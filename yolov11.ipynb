{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Data Generation for YOLOv7+ with VOCDataset and Training of YOLOv12\n",
    "\n",
    "1. Take pictures\n",
    "1. Prepare environment\n",
    "1. Prepare object images\n",
    "1. Organize the object images\n",
    "3. Download VOCDataset for backing of object images\n",
    "1. Overlay objects on backing images and generate YOLO labels\n",
    "9000. Train model\n",
    "1. Upload to Luxonis to convert to OpenVINO format\n",
    "\n",
    "Note: the `os.chdir()` function at the beginning of most code cells is necessary for users to know what directory they're in, and to make sure functions are being executed in the correct directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Take Pictures\n",
    "\n",
    "When taking pictures of objects:\n",
    "- TAKE MANY PHOTOS AT ALL POSSIBLE ANGLES (30-ish photos depending on how many angles we need)\n",
    "- Good lighting and quality, of course\n",
    "- Try to have the object be the only thing in frame (no extra objects that could possibly take away focus from the image)\n",
    "- The object should be completely in frame and not cut off\n",
    "- When you get all these images, place them in `~/uneditedObjects/`\n",
    "- THEY HAVE TO BE PNGs FOR PILLOW TO MODIFY THEM\n",
    "\n",
    "Examples of images:\n",
    "\n",
    "<img src=\"markdownimages/uneditedimage.png\" alt=\"goodexample-baseimage\" width=\"900\" height=\"300\">\n",
    "\n",
    "## 2. Prepare Environment\n",
    "\n",
    "In Visual Studio Code, run the below commands to make a virtual environment and get ultralytics and other packages:\n",
    "\n",
    "**(Once you are in the correct directory, run this cell AGAIN to recreate variables since they get restarted)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell block must be run first before anything else because the directory system is confusing and we often have trouble tracking which directory we're in\n",
    "import os\n",
    "absoluteMVpath = os.getcwd()\n",
    "#!python3.10 --version\n",
    "# check if python 3.10 is installed to create the 3.10 venv (YOU CANNOT USE ANOTHER VERSION)\n",
    "!python3.10 -m venv .venv\n",
    "!source .venv/bin/activate\n",
    "# once the venv is created, find the VS-Code kernel selection and set it to the venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "absoluteMVpath = os.getcwd()\n",
    "os.chdir(absoluteMVpath)\n",
    "# you need to import os again if you change the kernel :)\n",
    "!git clone --progress --verbose https://github.com/ultralytics/ultralytics\n",
    "\n",
    "# we do not cd into ultralytics yet since we complete other object image tasks first\n",
    "# operations are inside ultralytics\n",
    "# create other folders that we will need later:\n",
    "\n",
    "# -p checks if the directory already exists before making it\n",
    "!mkdir -p ./ultralytics/objectImages\n",
    "!mkdir -p ./ultralytics/ultralytics/Dataset\n",
    "!mkdir -p ./ultralytics/ultralytics/Dataset/images\n",
    "!mkdir -p ./ultralytics/ultralytics/Dataset/labels\n",
    "!mkdir -p ./ultralytics/ultralytics/Dataset/images/test\n",
    "!mkdir -p ./ultralytics/ultralytics/Dataset/images/train\n",
    "!mkdir -p ./ultralytics/ultralytics/Dataset/labels/test\n",
    "!mkdir -p ./ultralytics/ultralytics/Dataset/labels/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CUDA version if you have CUDA for pytorch installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for yolov11 and all other ultralytics yolo versions:\n",
    "\n",
    "For pytorch: \n",
    "**Visit [pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/) to install the correct version of pytorch** (only change the url part of the command and if you have a newer version of CUDA than pytorch has then install the latest pytorch version). This could take a while depending on your network speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Machines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"ultralytics\" \"tqdm>=4.41.0\" \"pillow\" \"pip install rembg[gpu,cli]==2.0.28\"\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-CUDA Machines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"ultralytics\" \"tqdm>=4.41.0\" \"pillow\" \"rembg==2.0.28\"\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Object Images\n",
    "\n",
    "**The below code edits the images for you**, however here are instructions for how to edit them manually if needed:\n",
    "\n",
    "- when you have your images head over to [remove.bg](https://remove.bg) and get the transparent background edits to your images\n",
    "- open up each image without a background to GIMP or your photo editor of choice\n",
    "- crop the images until you are right at the edge of them down to the pixel. \n",
    "\n",
    "Good example: (note how there is no background and it is cropped right to the edge of the object)\n",
    "\n",
    "<img src=\"markdownimages/editedimage.png\" alt=\"goodexample-editedimage\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(absoluteMVpath)\n",
    "from PIL import Image\n",
    "# we first use rembg to remove background of images from our base images\n",
    "# THEN we put them through Pillow to crop the images using the detected removed background\n",
    "# then it saves it in ultralytics\n",
    "\n",
    "!mkdir ./temporary/\n",
    "!rembg p ./uneditedObjects temporary\n",
    "\n",
    "for i in os.listdir('./temporary/'):\n",
    "    img = Image.open(f\"./temporary/{i}\").convert(\"RGBA\") # MUST BE PNG FILE TO USE RGBA >:/\n",
    "    alpha = img.split()[3] # alpha channel is apparently the 4th channel in RGBA, it means transparency\n",
    "    # threshold makes sure that even if there are translucent pixels in the background we crop them\n",
    "    threshold = 10  # tweak this value if needed (0â€“255), 10 is pretty good for now\n",
    "    alpha = alpha.point(lambda p: p > threshold and 255)\n",
    "    # alpha.point is a function applied to all pixels in an image\n",
    "    # the threshold checks how translucent it is and either makes it fully transparent or opaque\n",
    "    img.putalpha(alpha) # recombine to an RGBA image\n",
    "    bbox = alpha.getbbox() # find the bounding box of non-transparent pixels (find the edge of where there actually is an image object)\n",
    "\n",
    "    if bbox:\n",
    "        cropped = img.crop(bbox)\n",
    "        cropped.save(f\"./ultralytics/objectImages/{i}\") # the Final Destination (haha) of the images\n",
    "\n",
    "!rm -rf ./temporary/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Organize Object Images\n",
    "Currently the file structure of important files should look like:\n",
    "```\n",
    "ultralytics\n",
    "â”œâ”€â”€ ultralytics\n",
    "|     â”œâ”€â”€ Dataset\n",
    "|     |     â”œâ”€â”€ images\n",
    "|     |     â””â”€â”€ labels\n",
    "|     â””â”€â”€ (folders with all of the yolo code and other things we will get to later)\n",
    "â”œâ”€â”€ objectImages\n",
    "â””â”€â”€ (more yolo files and folders)\n",
    "```\n",
    "\n",
    "Your images are currently in `~/MonsterVision5/YOLO/ultralytics/objectImages/`. Rename all of the images there to `*class*_*number*`<br>\n",
    "\n",
    "ex: `blue_1`, `cone_29`, `banana_0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now edit the list below to contain all of your class names. We will need this later for generating the dataset and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(absoluteMVpath)\n",
    "classNames = [\"algae\"] # Edit to have the correct name of the object\n",
    "import os\n",
    "import json\n",
    "CLASSES = {key: index for index, key in enumerate(classNames)} # DON'T EDIT\n",
    "os.environ['CLASSES'] = json.dumps(CLASSES)\n",
    "print(json.dumps(CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download VOCDataset for Backing of Object Images\n",
    "We use the VOCDataset or Visualized Object Classes Dataset which is a dataset that contains many images with labels for training of pascal. We are just extracting the images from a few of the datasets for backing images for our yolo training images.\n",
    "\n",
    "Download the 2007 and 2012 VOCDataset and put them in a separate directory (may take 10+ minutes depending on wifi) and extract the downloaded .tar archive files (should create folders `VOC2007` and `VOC2012` in `VOCdevkit`. If not then run again):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(absoluteMVpath + \"/ultralytics\")\n",
    "!mkdir -p ./VOCdevkit \n",
    "!mkdir -p ./VOCdevkit/test\n",
    "!mkdir -p ./VOCdevkit/train\n",
    "# -p (lowercase) checks if the directory already exists before making\n",
    "\n",
    "# if running on Mac, you will only be able to use !wget if you have it installed already.\n",
    "!wget -P ./VOCdevkit https://www.kaggle.com/api/v1/datasets/download/gopalbhattrai/pascal-voc-2012-dataset\n",
    "\n",
    "!unzip ./VOCdevkit/pascal-voc-2012-dataset -d ./VOCdevkit/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mac:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(absoluteMVpath + \"/ultralytics\")\n",
    "!mkdir -p ./VOCdevkit \n",
    "!mkdir -p ./VOCdevkit/test\n",
    "!mkdir -p ./VOCdevkit/train\n",
    "# -p (lowercase) checks if the directory already exists before making\n",
    "# the curl command is pre-installed on Mac\n",
    "!curl -L -o ./VOCdevkit/pascal-voc-2012-dataset https://www.kaggle.com/api/v1/datasets/download/gopalbhattrai/pascal-voc-2012-dataset\n",
    "\n",
    "!unzip ./VOCdevkit/pascal-voc-2012-dataset -d ./VOCdevkit/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(absoluteMVpath + \"/ultralytics\")\n",
    "!mkdir -p ./VOCdevkit\n",
    "!mkdir -p ./VOCdevkit/test\n",
    "!mkdir -p ./VOCdevkit/train\n",
    "\n",
    "# -p checks if the directory already exists before making\n",
    "!curl -L -o ./VOCdevkit/pascal-voc-2012-dataset https://www.kaggle.com/api/v1/datasets/download/gopalbhattrai/pascal-voc-2012-dataset\n",
    "!tar -xvf ./VOCdevkit/pascal-voc-2012-dataset.zip -C ./VOCdevkit/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move the necessary files to ultralytics for training later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(absoluteMVpath + \"/ultralytics/VOCdevkit\")\n",
    "for i in os.listdir('./VOC2012_test/VOC2012_test/JPEGImages'): # listdir wil return a list of strings of every file and folder name in the DIR\n",
    "    os.rename('./VOC2012_test/VOC2012_test/JPEGImages/' + i, './test/' + i) # takes original path and appends each image file then moves it by \"renaming\" it\n",
    "for i in os.listdir('./VOC2012_train_val/VOC2012_train_val/JPEGImages'):\n",
    "    os.rename('./VOC2012_train_val/VOC2012_train_val/JPEGImages/' + i, './train/' + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, clean up your files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linux/Mac:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(absoluteMVpath + \"/ultralytics/VOCdevkit\")\n",
    "!rm -rf ./VOC2012_test/\n",
    "!rm -rf ./VOC2012_train_val/\n",
    "!rm pascal-voc-2012-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(absoluteMVpath + \"/ultralytics/VOCdevkit\")\n",
    "!rmdir ./VOC2012_test/\n",
    "!rmdir ./VOC2012_train_val/\n",
    "!del pascal-voc-2012-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overlay objects on backing images and generate YOLO labels\n",
    "\n",
    "This script automatically generates synthetic training data for object detection models (such as YOLO) by compositing foreground object images onto random background images.\n",
    "\n",
    "### Overview:\n",
    "1. Randomly selects an object image (with transparent background) and a random background image.\n",
    "2. Scales the object to a random percentage of the backgroundâ€™s size.\n",
    "3. Pastes the scaled object onto a random position within the background.\n",
    "4. Saves the resulting composite image to the dataset folder (e.g., ./ultralytics/Dataset/images/...).\n",
    "5. Generates a corresponding YOLO-format label (.txt) containing the objectâ€™s class ID and bounding box coordinates\n",
    "   (normalized x_center, y_center, width, height).\n",
    "6. Designed for multiprocessing to efficiently generate large datasets.\n",
    "\n",
    "### Functions:\n",
    "- stackAndScaleImage(): Scales and pastes one image onto another using PIL.\n",
    "- selectScaleAndCreateYoloLabels(): Combines object and background, computes YOLO label data.\n",
    "- combineRandomImages(): Picks random images from directories and combines them.\n",
    "- makeImage(): Creates and saves a single labeled synthetic training image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(absoluteMVpath + \"/ultralytics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./dataGen.py \n",
    "# we write to a new file so we can do multiprocessing on a separate python file (we cant do multiprocessing in .ipynb)\n",
    "from PIL import Image\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm as progressBar\n",
    "import threading\n",
    "import multiprocessing\n",
    "import concurrent.futures\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "def stackAndScaleImage(objectImage, backgroundImage, scalePercent, position): # Takes a first PNG image, scales it down a certain percentage and pastes it on a second PNG image.\n",
    "\n",
    "# objectImage: PIL image of object\n",
    "# backgroundImage: PIL image of background\n",
    "# scalePercent: Percentage to scale down the first image. (MAKE FLOAT INSTEAD OF DUMB STUFF)\n",
    "# position: Tuple of (x, y) coordinates to paste the scaled image.\n",
    "\n",
    "  objectImage = Image.open(objectImage)\n",
    "  backgroundImage = Image.open(backgroundImage)\n",
    "\n",
    "  # Scale down the first image\n",
    "  width, height = objectImage.size # extract the width and height of object\n",
    "  newWidth = int(width * scalePercent) # create a new width for object based on the scalePercent and makes it an int\n",
    "  newHeight = int(height * scalePercent) # create a new height for object based on the scalePercent and makes it an int\n",
    "  objectScaled = objectImage.resize((newWidth, newHeight)) # use the resize method of a PIL Image to scale object to the new_width and new_height\n",
    " \n",
    "  # Scaled the hight to add more variation to the data\n",
    "  width, height = objectScaled.size # Reset the width and height variables to be the new width and height of object after scaling\n",
    "  randomHeightScale = random.uniform(0.8, 1.2) # Choose a random scalePercent between the minimum and maximum values.\n",
    "  # randomHeightScale = 1.0\n",
    "  newHeight = int(height * randomHeightScale) # Calculate the newHeight based on the random scalePercent above\n",
    "  if newHeight > backgroundImage.height:\n",
    "    newHeight = backgroundImage.height\n",
    "    print(\"triggered\", newHeight)\n",
    "  objectScaled = objectScaled.resize((width, newHeight)) # Resize the image just like above\n",
    "  \n",
    "  backgroundImage.paste(objectScaled, position, objectScaled) # Second parameter makes it so that the pixels with no value meant to be clear stay clear and aren't black\n",
    "\n",
    "  return (backgroundImage, newHeight) # Return the final stacked image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SCALE PERCENTAGE IS HOW MUCH OF THE FRAME YOU WANT TO TAKE UP NOT HOW MUCH YOU WANT TO SCALE THE OBJECT IMAGE DOWN\n",
    "def selectScaleAndCreateYoloLabels(objectPath, backgroundPath, minSizePercent, maxSizePercent, objectClassStr, CLASSES): \n",
    "  # Combines two images by pasting a scaled version of an object onto a background. SCALES BASED ON WIDTH\n",
    "\n",
    "  # objectPath: Path to the object image.\n",
    "  # backgroundPath: Path to the background image.\n",
    "  # minSizePercent: Minimum percentage to scale down object's WIDTH.\n",
    "  # maxSizePercent: Maximum percentage to scale down object's WIDTH.\n",
    "\n",
    "  object = Image.open(objectPath) # Open the first image as object\n",
    "  background = Image.open(backgroundPath) # Open the second image as background\n",
    "\n",
    "  objectWidth, objectHeight = object.size # Extract the width and height of object as objectWidth and objectHeight\n",
    "  backgroundWidth, backgroundHeight = background.size # Extract the width and height of background as backgroundWidth and backgroundHeight\n",
    "\n",
    "  # Choose a random scalePercent between the minimum and maximum values provided as parameters\n",
    "  # NEVER ABOVE 1.0(?)\n",
    "  scalePercent = random.uniform(minSizePercent, maxSizePercent)\n",
    "\n",
    "  # as background's width before applying this random scale percent (should really be same as shortest side?)\n",
    "  # baseScale = backgroundWidth / objectWidth # The baseScale is the ratio of how big background's width is compared to object's width (ONLY WITH MEASUREMENTS)\n",
    "  if backgroundWidth <= backgroundHeight: # scale based on shortest side of background\n",
    "    baseScale = backgroundWidth/objectWidth\n",
    "    used = \"width shortest so base scale is based on width\"\n",
    "  else:\n",
    "    baseScale = backgroundHeight/objectHeight\n",
    "    used = \"height shortest so base scale is based on height\"\n",
    "\n",
    "  # print({\n",
    "  #   \"backgroundWidth\": backgroundWidth,\n",
    "  #   \"object.width\": object.width,\n",
    "  #   \"objectWidth\": objectWidth,\n",
    "  #   \"baseScale\": baseScale,\n",
    "  #   \"scalePercent\": scalePercent,\n",
    "  #   \"baseScale * scalePercent\": baseScale * scalePercent,\n",
    "  #   \"object.width * scalePercent/100\": object.width * scalePercent/100,\n",
    "  #   \"object.height * scalePercent/100\": object.height * scalePercent/100,\n",
    "  # })\n",
    "\n",
    "\n",
    "  scalePercent = baseScale * scalePercent # fix the scalePercent to include the baseScale between the 2 images and be proportional\n",
    "\n",
    "  # Choose a random position for object in background\n",
    "  widthOfObjectAfterScaling = int(object.width * scalePercent)   # predict width of object after scaling so you can choose a random width in bounds of background\n",
    "  heightOfObjectAfterScaling = int(object.height * scalePercent) # predict height of object after scaling so you can choose a random height in bounds of background\n",
    "  x = random.randint(0, backgroundWidth - widthOfObjectAfterScaling)     # select random x position for object in background\n",
    "\n",
    "  # ERROR IS HERE: The error is because background isn't tall enough to fit object even after scaling so the randint is trying to selced from 0 to a negative number\n",
    "  y = random.randint(0, backgroundHeight - heightOfObjectAfterScaling)   # select random y position for object in background\n",
    "\n",
    "  (combinedImage, finalHeight) = stackAndScaleImage(objectPath, backgroundPath, scalePercent, (x, y)) # Use the stack_scaled_images function to combine the two images\n",
    "\n",
    "  # finalWidth, finalHeight = combinedImage.size # WRONG\n",
    "\n",
    "  # Save the paste_parameters as a json object\n",
    "  debugParameters = {\n",
    "    \"width_background\": backgroundWidth,\n",
    "    \"height_background\": backgroundHeight,\n",
    "    \"paste_x\": x,\n",
    "    \"paste_y\": y,\n",
    "    \"paste_width\": widthOfObjectAfterScaling,\n",
    "    \"paste_height\": finalHeight,\n",
    "    \"scale_type\": used\n",
    "  }\n",
    "  # The position for object to be pasted onto background is randomly selected within the bounds of background\n",
    "\n",
    "  # https://docs.cogniflow.ai/en/article/how-to-create-a-dataset-for-object-detection-using-the-yolo-labeling-format-1tahk19/\n",
    "  # YOLO labeling parameters\n",
    "  pasteParametersYolo = {\n",
    "    \"objectClassNum\": CLASSES[objectClassStr],\n",
    "    \"x_center\": (x + widthOfObjectAfterScaling/2.0) / backgroundWidth, # calculate what percentage of the width of background the center of scaled object will be\n",
    "    \"y_center\": (y + finalHeight/2.0) / backgroundHeight, # calculate what percentage of the height of background the center of scaled object will be\n",
    "    \"width\": widthOfObjectAfterScaling / backgroundWidth, # calculate what percentage of the width of background does scaled object take\n",
    "    \"height\": finalHeight / backgroundHeight, # calculate what percentage of the height of background does scaled object take\n",
    "  }\n",
    "\n",
    "  return (combinedImage, debugParameters, pasteParametersYolo) # return the PIL image object after stacking, the parameters used for pasting, and the parameters used for pasting in a YOLO suitable notation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def combineRandomImages(directory1, directory2, minSizePercent, maxSizePercent, CLASSES):\n",
    "  # Combines two random images from the two directories using the first function, returns a PIL Image object with the combined images\n",
    "\n",
    "  #   directory1: Path to the first directory.\n",
    "  #   directory2: Path to the second directory.\n",
    "  #   minSizePercent: Minimum percentage to scale down the first image.\n",
    "  #   maxSizePercent: Maximum percentage to scale down the first image.\n",
    "\n",
    "  # Get all files in directories. If one isn't an image it could break\n",
    "  objectFiles = os.listdir(directory1) # Get a list of all files in the first directory\n",
    "  backgroundFiles = os.listdir(directory2) # Get a list of all files in the second directory\n",
    "\n",
    "  imageChosen = random.choice(objectFiles)\n",
    "  objectClass = imageChosen.split(\"_\")[0]\n",
    "\n",
    "  # Choose random images\n",
    "  objectPath = os.path.join(directory1, imageChosen) # Choose a random file from the first directory\n",
    "  backgroundPath = os.path.join(directory2, random.choice(backgroundFiles)) # Choose a random file from the second directory\n",
    "\n",
    "  # Use the combine_images_james_xy function to combine the two images.\n",
    "  combinedImage, debugParameters, pasteParametersYolo = selectScaleAndCreateYoloLabels(objectPath, backgroundPath, minSizePercent, maxSizePercent, objectClass, CLASSES)\n",
    "\n",
    "  # print(pasteParametersYolo)\n",
    "\n",
    "  return (combinedImage, debugParameters, pasteParametersYolo) # re-return all of the returns from the combine_images_james_xy function\n",
    "\n",
    "# def makeImage(testOrTrain=\"test\", numImages=10, minSizePercent=.05, maxSizePercent=.8, i=-1):\n",
    "def makeImage(args):\n",
    "  (testOrTrain, numImages, minSizePercent, maxSizePercent, CLASSES, i) = args\n",
    "  if i == -1:\n",
    "    raise ValueError(\"Invalid i Value\")\n",
    "  # Combine the images from directory1 (game object) with images from directory2 (backgrounds).\n",
    "  combinedImage, debugParameters, pasteParametersYolo = combineRandomImages(\"./objectImages\", \"./VOCdevkit/\"+testOrTrain, minSizePercent, maxSizePercent, CLASSES)\n",
    "  # Figure out a file name based on the current iteration and type of dataset\n",
    "  baseFilename = f\"{testOrTrain}_{i:0{6}d}\" # Max 100000 file names\n",
    "  # Save the image to the specified folder based on type of data set and use the above created filename\n",
    "  combinedImage.save('./ultralytics/Dataset/images/'+testOrTrain+'/'+baseFilename+'.png')\n",
    "\n",
    "  # Open/create a text file with the same name as the image and add the paste_parameters_yolo to it\n",
    "  with open('./ultralytics/Dataset/labels/'+testOrTrain+'/'+baseFilename+'.txt', \"w\") as f:\n",
    "    yoloData = pasteParametersYolo\n",
    "    if round(yoloData['x_center'],6) > 1 or round(yoloData['y_center'],6) > 1:\n",
    "      print(\"ERROR at\", i)\n",
    "      print(yoloData)\n",
    "      print(debugParameters)\n",
    "    f.write(f\"{yoloData['objectClassNum']} {round(yoloData['x_center'],6)} {round(yoloData['y_center'],6)} {round(yoloData['width'],6)} {round(yoloData['height'],6)}\") # write data and round it to the correct decimal places (first digit in string is the class)\n",
    "\n",
    "\n",
    "\n",
    "# Main function that ties together all of the other functions to make it work\n",
    "def makeData(testOrTrain=\"test\", numImages=10, minSizePercent=.05, maxSizePercent=.8):\n",
    "  args = tuple([(testOrTrain, numImages, minSizePercent, maxSizePercent, CLASSES, i) for i in range(numImages)]) # creates a tuple of tuple\n",
    "\n",
    "  with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    executor.map(makeImage, args)\n",
    "    print(\"yippie!!! it works\") # to make sure it works :)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\"--classes\", type=str)\n",
    "  parser.add_argument(\"--testOrTrain\", default=\"test\", type=str)\n",
    "  parser.add_argument(\"--numImages\", default=2, type=int)\n",
    "  parser.add_argument(\"--minSizePercent\", default=.05, type=float)\n",
    "  parser.add_argument(\"--maxSizePercent\", default=.8, type=float)\n",
    "\n",
    "  inputArgs = parser.parse_args()\n",
    "\n",
    "  CLASSES = json.loads(inputArgs.classes.replace(\"'\", '\"'))\n",
    "  testOrTrain = inputArgs.testOrTrain\n",
    "  numImages = inputArgs.numImages\n",
    "  minSizePercent = inputArgs.minSizePercent\n",
    "  maxSizePercent = inputArgs.maxSizePercent\n",
    "\n",
    "  testOrTrain = testOrTrain.lower()\n",
    "\n",
    "  makeData(testOrTrain, numImages, minSizePercent, maxSizePercent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the test and train image sets. You should pick a size percent that makes sense for the application. I suggest doing some tests by just generating 50 images and then seeing how it looks. Here are the lines to make a test and train dataset:\n",
    "\n",
    "(This is going to take a few minutes on some machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train code\n",
    "os.chdir(absoluteMVpath + \"/ultralytics\")\n",
    "!python3 dataGen.py --classes=\"$CLASSES\" --testOrTrain=\"train\" --numImages=15000 --minSizePercent=.1 --maxSizePercent=.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "os.chdir(absoluteMVpath + \"/ultralytics\")\n",
    "!python3 dataGen.py --classes=\"$CLASSES\" --testOrTrain=\"test\" --numImages=7500 --minSizePercent=.1 --maxSizePercent=.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the dataset config file for training yolo but **make sure to enumerate the classes at the bottom and pick names that are appropriate**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./ultralytics/cfg/datasets/CUSTOM.yaml\n",
    "# Ultralytics YOLO ðŸš€, AGPL-3.0 license\n",
    "# Documentation: # Documentation: https://docs.ultralytics.com/datasets/detect/voc/\n",
    "# Example usage: yolo train data=CUSTOM.yaml\n",
    "# parent\n",
    "# â”œâ”€â”€ ultralytics\n",
    "# â””â”€â”€ datasets\n",
    "#     â””â”€â”€ CUSTOM\n",
    "\n",
    "# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\n",
    "path: ./ultralytics/Dataset/\n",
    "train: images/train # train images (relative to 'path')  16551 images\n",
    "val: images/test # val images (relative to 'path')  4952 images\n",
    "test: images/test # test images (optional)\n",
    "\n",
    "# Set the number of classes\n",
    "nc: 1 # EDIT TO REFLECT CORRECT VALUES\n",
    "\n",
    "# Classes\n",
    "names: # EDIT HERE\n",
    "  0: algae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model\n",
    "\n",
    "Now that we have the dataset we can train the model. First we will run the checks command to make sure everything is good (it has all the libraries, the computer is able to train it, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm we have pyTorch GPU training available on our system (prints as a boolean)\n",
    "\n",
    "Please pay attention if it is false!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is good we will run the help command to see how to use the yolo command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the nano version of yolov11 because it is the smallest, fastest, and most lightweight version. We will specify the name of the custom data config file that we created above as well as yolov11n.pt for the nano version of the pretrained model. I've decided on 10 epochs for just a short retraining of the model. The imgsz is 640 which will resize all of the images we created to 640x640 for input into the model. This number can be changed for speed over accuracy if needed. I set the batch size to use 90% of the available GPU memory as well as set cache to true to improve speed.\n",
    "\n",
    "--------\n",
    "\n",
    "my face when yolov12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo settings datasets_dir=ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i am a genuous....\n",
    "\n",
    "from ultralytics import settings\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imgsz is set to 512 because it must be a multiple of 32 and not greater than 530 to work with the camera with the OV... sensor\n",
    "\n",
    "*****If the boolean value a few cells above for CUDATorch returned False,***** set device=cpu. otherwise keep it at device=0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---------\n",
    "\n",
    "\n",
    "KEEP IN MIND:\n",
    "\n",
    "if you didn't already know, training takes a while (as in I started training at around 1 am. it is now 8 am). Depending on what system you're on, it will probably take less long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo detect train data=\"./ultralytics/cfg/datasets/CUSTOM.yaml\" model=\"yolo12n.pt\" epochs=2 imgsz=512 name=\"lakeMONSTER\" batch=0.5 patience=2 cache=disk workers=8 device=0 project=./datagenTraining/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Upload to Luxonis to convert to OpenVINO format\n",
    "\n",
    "### NOTE: tools.luxonis.com is no longer maintained and is not compatible with versions past Yolov11 or depthAIv3.\n",
    "(How unfortunate that those are both versions we want to upgrade our code to...)\n",
    "\n",
    "-------\n",
    "\n",
    "Navigate to [tools.luxonis.com](https://tools.luxonis.com) and select the following parameters:\n",
    "\n",
    "- Yolo Version: `YOLOv11`\n",
    "- `RVC2`\n",
    "- Upload your best.pt file (when you finish running the above code it prints where the files are)\n",
    "  - We use best.pt because it is literally the best out of the rest... last.pt is just the last trained epoch, since there may be situations where too much training actually makes it worse, so the best isn't one of the last few epochs. Most of the time they're pretty similar but we should use best.pt\n",
    "- Image shape: `512`\n",
    "- Shaves: `6`\n",
    "- Use OpenVINO: `True`\n",
    "\n",
    "When it finishes processing, you will get a `result.zip` file. Open it and replace everything in the `models` folder with the content of the `result` folder.\n",
    "\n",
    "## End\n",
    "You are done with this notebook! The next steps of using the models come in after you set up MonsterVision on the robot. Refer to the README.md for more clear instructions on when the models come in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
