{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Data Generation for YOLOv11 with VOCDataset and Training\n",
    "\n",
    "1. Take pictures\n",
    "1. Prepare environment\n",
    "1. Prepare/Organize object images\n",
    "3. Download VOCDataset for backing of object images\n",
    "1. Overlay objects on backing images and generate YOLO labels\n",
    "9000. Train model\n",
    "1. Upload to Luxonis to convert to OpenVINO format\n",
    "\n",
    "--------\n",
    "\n",
    "## 1. Take Pictures\n",
    "\n",
    "When taking pictures of objects:\n",
    "- TAKE MANY PHOTOS AT ALL POSSIBLE ANGLES (15-ish photos, depending on how many angles we need)\n",
    "- Good lighting and quality, of course\n",
    "- Try to have the object be the only thing in frame (no extra objects that could possibly take away focus from the image)\n",
    "- The object should be completely in frame and not cut off\n",
    "- When you get all these images, place them in `~/MonsterVision5/uneditedObjects/` (you may have to manually create the folder if it does not exist)\n",
    "\n",
    "Examples of some image angles/variation:\n",
    "\n",
    "<img src=\"markdownimages/uneditedimage.png\" alt=\"goodexample-baseimage\" width=\"900\" height=\"300\">\n",
    "\n",
    "For testing purposes, there are some sample images in the `sample-images/` folder with algaes, corals, notes, and carrots, you can duplicate the contents (but do not remove them!)\n",
    "\n",
    "--------\n",
    "\n",
    "## 2. Prepare Environment\n",
    "\n",
    "\n",
    "### IMPORTANT THINGS TO NOTE WHEN RUNNING THIS BLOCK:\n",
    "1. this cell block must be run first before anything else because it creates our venv!!!\n",
    "1. If you don't have a kernel already selected, select `python version 3.10` (WE NEED THIS VERSION OF PYTHON)\n",
    "\n",
    "Note on linux, you may need to install python3.10 as follows before running this block:\n",
    "\n",
    "```bash\n",
    "# 1. Update your package list and install the prerequisite for adding PPAs\n",
    "sudo apt update\n",
    "sudo apt install software-properties-common -y\n",
    "\n",
    "# 2. Add the deadsnakes PPA (Press Enter when prompted)\n",
    "sudo add-apt-repository ppa:deadsnakes/ppa\n",
    "\n",
    "# 3. Update the package list again to include the new PPA repositories\n",
    "sudo apt update\n",
    "\n",
    "# 4. Install Python 3.10 and the venv module\n",
    "sudo apt install python3.10 python3.10-venv -y\n",
    "```\n",
    "\n",
    "<img src=\"markdownimages/image.png\" alt=\"goodexample-baseimage\" width=\"550\" height=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darwin\n",
      "mac/linux user\n",
      "Python 3.10.18\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.system())\n",
    "import shutil\n",
    "import sys\n",
    "isWindows = False\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    # if machine is windows\n",
    "    isWindows = True\n",
    "    print(\"the next line should print 3.10, otherwise you are using the wrong version!!!!\")\n",
    "    !python --version\n",
    "    !python -m venv .venv\n",
    "    !.\\.venv\\Scripts\\activate\n",
    "    print(\"windows user!\")\n",
    "else:\n",
    "    # if machine is NOT!!!! windows\n",
    "    isWindows = False\n",
    "    print (\"mac/linux user\")\n",
    "\n",
    "    # Check if python3.10 exists in the system PATH\n",
    "    if shutil.which(\"python3.10\") is None:\n",
    "        print(\"ERROR: python3.10 is not installed or not in PATH.\")\n",
    "        print(\"Please install python3.10 before running this script.\")\n",
    "        sys.exit(1)  # Terminate the script with an error code\n",
    "    \n",
    "    !python3.10 --version\n",
    "    !python3.10 -m venv .venv\n",
    "    !source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after running the code block, you will now have a venv! change the kernel again to python VENV 3.10, and only then can you run this next block\n",
    "\n",
    "this is also something that MUST be run before running the subsequent code blocks, but then again youre supposed to be running all these in order either way :)\n",
    "\n",
    "<img src=\"markdownimages/image2.png\" alt=\"goodexample-baseimage\" width=\"900\" height=\"300\">\n",
    "\n",
    "#### Press \"Install\" if it asks to install `ipykernel`!!! ^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CUDA version if you have CUDA for pytorch installation and install dependencies for yolov11 and all other ultralytics yolo versions:\n",
    "\n",
    "For pytorch: \n",
    "**Visit [pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/) to install the correct version of pytorch** (only change the url part of the command and if you have a newer version of CUDA than pytorch has then install the latest pytorch version). This could take a while depending on your network speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darwin\n",
      "mac/linux user\n",
      "Cloning into 'ultralytics'...\n",
      "POST git-upload-pack (182 bytes)\n",
      "POST git-upload-pack (gzip 42559 to 21449 bytes)\n",
      "remote: Enumerating objects: 77863, done.\u001b[K\n",
      "remote: Counting objects: 100% (639/639), done.\u001b[K\n",
      "remote: Compressing objects: 100% (305/305), done.\u001b[K\n",
      "remote: Total 77863 (delta 535), reused 334 (delta 334), pack-reused 77224 (from 5)\u001b[K\n",
      "Receiving objects: 100% (77863/77863), 42.26 MiB | 45.17 MiB/s, done.\n",
      "Resolving deltas: 100% (58395/58395), done.\n",
      "Collecting ultralytics\n",
      "  Using cached ultralytics-8.3.241-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting tqdm>=4.41.0\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pillow\n",
      "  Using cached pillow-12.0.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
      "Collecting rembg==2.0.28\n",
      "  Using cached rembg-2.0.28-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting onnx\n",
      "  Using cached onnx-1.20.0-cp310-cp310-macosx_12_0_universal2.whl.metadata (8.4 kB)\n",
      "Collecting blobconverter\n",
      "  Using cached blobconverter-1.4.3-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting aiohttp==3.8.1 (from rembg==2.0.28)\n",
      "  Using cached aiohttp-3.8.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting asyncer==0.0.2 (from rembg==2.0.28)\n",
      "  Using cached asyncer-0.0.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting click==8.1.3 (from rembg==2.0.28)\n",
      "  Using cached click-8.1.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting fastapi==0.87.0 (from rembg==2.0.28)\n",
      "  Using cached fastapi-0.87.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting filetype==1.2.0 (from rembg==2.0.28)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pooch==1.6.0 (from rembg==2.0.28)\n",
      "  Using cached pooch-1.6.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting imagehash==4.3.1 (from rembg==2.0.28)\n",
      "  Using cached ImageHash-4.3.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting numpy==1.23.5 (from rembg==2.0.28)\n",
      "  Using cached numpy-1.23.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.3 kB)\n",
      "Collecting onnxruntime==1.13.1 (from rembg==2.0.28)\n",
      "  Using cached onnxruntime-1.13.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting opencv-python-headless==4.6.0.66 (from rembg==2.0.28)\n",
      "  Using cached opencv_python_headless-4.6.0.66-cp37-abi3-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting pillow\n",
      "  Using cached Pillow-9.3.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting pymatting==1.1.8 (from rembg==2.0.28)\n",
      "  Using cached PyMatting-1.1.8-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting python-multipart==0.0.5 (from rembg==2.0.28)\n",
      "  Using cached python_multipart-0.0.5-py3-none-any.whl\n",
      "Collecting scikit-image==0.19.3 (from rembg==2.0.28)\n",
      "  Using cached scikit_image-0.19.3-cp310-cp310-macosx_12_0_arm64.whl.metadata (8.0 kB)\n",
      "Collecting scipy==1.9.3 (from rembg==2.0.28)\n",
      "  Using cached scipy-1.9.3-cp310-cp310-macosx_12_0_arm64.whl.metadata (53 kB)\n",
      "Collecting tqdm>=4.41.0\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl.metadata (57 kB)\n",
      "Collecting uvicorn==0.20.0 (from rembg==2.0.28)\n",
      "  Using cached uvicorn-0.20.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting watchdog==2.1.9 (from rembg==2.0.28)\n",
      "  Using cached watchdog-2.1.9-cp310-cp310-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp==3.8.1->rembg==2.0.28)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting charset-normalizer<3.0,>=2.0 (from aiohttp==3.8.1->rembg==2.0.28)\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp==3.8.1->rembg==2.0.28)\n",
      "  Using cached multidict-6.7.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp==3.8.1->rembg==2.0.28)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp==3.8.1->rembg==2.0.28)\n",
      "  Using cached yarl-1.22.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (75 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp==3.8.1->rembg==2.0.28)\n",
      "  Using cached frozenlist-1.8.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp==3.8.1->rembg==2.0.28)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting anyio<4.0.0,>=3.4.0 (from asyncer==0.0.2->rembg==2.0.28)\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 (from fastapi==0.87.0->rembg==2.0.28)\n",
      "  Using cached pydantic-1.10.26-cp310-cp310-macosx_11_0_arm64.whl.metadata (155 kB)\n",
      "Collecting starlette==0.21.0 (from fastapi==0.87.0->rembg==2.0.28)\n",
      "  Using cached starlette-0.21.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting PyWavelets (from imagehash==4.3.1->rembg==2.0.28)\n",
      "  Using cached pywavelets-1.8.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting coloredlogs (from onnxruntime==1.13.1->rembg==2.0.28)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime==1.13.1->rembg==2.0.28)\n",
      "  Using cached flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from onnxruntime==1.13.1->rembg==2.0.28) (25.0)\n",
      "Collecting protobuf (from onnxruntime==1.13.1->rembg==2.0.28)\n",
      "  Using cached protobuf-6.33.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting sympy (from onnxruntime==1.13.1->rembg==2.0.28)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting appdirs>=1.3.0 (from pooch==1.6.0->rembg==2.0.28)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting requests>=2.19.0 (from pooch==1.6.0->rembg==2.0.28)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting numba!=0.49.0 (from pymatting==1.1.8->rembg==2.0.28)\n",
      "  Using cached numba-0.63.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in ./.venv/lib/python3.10/site-packages (from python-multipart==0.0.5->rembg==2.0.28) (1.17.0)\n",
      "Collecting networkx>=2.2 (from scikit-image==0.19.3->rembg==2.0.28)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting imageio>=2.4.1 (from scikit-image==0.19.3->rembg==2.0.28)\n",
      "  Using cached imageio-2.37.2-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tifffile>=2019.7.26 (from scikit-image==0.19.3->rembg==2.0.28)\n",
      "  Using cached tifffile-2025.5.10-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting h11>=0.8 (from uvicorn==0.20.0->rembg==2.0.28)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting idna>=2.8 (from anyio<4.0.0,>=3.4.0->asyncer==0.0.2->rembg==2.0.28)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting sniffio>=1.1 (from anyio<4.0.0,>=3.4.0->asyncer==0.0.2->rembg==2.0.28)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: exceptiongroup in ./.venv/lib/python3.10/site-packages (from anyio<4.0.0,>=3.4.0->asyncer==0.0.2->rembg==2.0.28) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in ./.venv/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp==3.8.1->rembg==2.0.28) (4.15.0)\n",
      "Collecting propcache>=0.2.1 (from yarl<2.0,>=1.0->aiohttp==3.8.1->rembg==2.0.28)\n",
      "  Using cached propcache-0.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting matplotlib>=3.3.0 (from ultralytics)\n",
      "  Using cached matplotlib-3.10.8-cp310-cp310-macosx_11_0_arm64.whl.metadata (52 kB)\n",
      "Collecting opencv-python>=4.6.0 (from ultralytics)\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-macosx_13_0_arm64.whl.metadata (19 kB)\n",
      "Collecting pyyaml>=5.3.1 (from ultralytics)\n",
      "  Using cached pyyaml-6.0.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting torch>=1.8.0 (from ultralytics)\n",
      "  Using cached torch-2.9.1-cp310-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting torchvision>=0.9.0 (from ultralytics)\n",
      "  Using cached torchvision-0.24.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: psutil>=5.8.0 in ./.venv/lib/python3.10/site-packages (from ultralytics) (7.2.0)\n",
      "Collecting polars>=0.20.0 (from ultralytics)\n",
      "  Using cached polars-1.36.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
      "  Using cached ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting ml_dtypes>=0.5.0 (from onnx)\n",
      "  Using cached ml_dtypes-0.5.4-cp310-cp310-macosx_10_9_universal2.whl.metadata (8.9 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached fonttools-4.61.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached kiwisolver-1.4.9-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Using cached pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Collecting llvmlite<0.47,>=0.46.0dev0 (from numba!=0.49.0->pymatting==1.1.8->rembg==2.0.28)\n",
      "  Using cached llvmlite-0.46.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv-python>=4.6.0 (from ultralytics)\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl.metadata (20 kB)\n",
      "Collecting polars-runtime-32==1.36.1 (from polars>=0.20.0->ultralytics)\n",
      "  Using cached polars_runtime_32-1.36.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->pooch==1.6.0->rembg==2.0.28)\n",
      "  Using cached urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->pooch==1.6.0->rembg==2.0.28)\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting filelock (from torch>=1.8.0->ultralytics)\n",
      "  Using cached filelock-3.20.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting jinja2 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime==1.13.1->rembg==2.0.28)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.13.1->rembg==2.0.28)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.8.0->ultralytics)\n",
      "  Using cached markupsafe-3.0.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Using cached rembg-2.0.28-py3-none-any.whl (12 kB)\n",
      "Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Using cached Pillow-9.3.0-cp310-cp310-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Using cached aiohttp-3.8.1-cp310-cp310-macosx_11_0_arm64.whl (552 kB)\n",
      "Using cached asyncer-0.0.2-py3-none-any.whl (8.3 kB)\n",
      "Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Using cached fastapi-0.87.0-py3-none-any.whl (55 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
      "Using cached numpy-1.23.5-cp310-cp310-macosx_11_0_arm64.whl (13.4 MB)\n",
      "Using cached onnxruntime-1.13.1-cp310-cp310-macosx_11_0_arm64.whl (5.4 MB)\n",
      "Using cached opencv_python_headless-4.6.0.66-cp37-abi3-macosx_11_0_arm64.whl (30.0 MB)\n",
      "Using cached pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "Using cached PyMatting-1.1.8-py3-none-any.whl (47 kB)\n",
      "Using cached scikit_image-0.19.3-cp310-cp310-macosx_12_0_arm64.whl (12.5 MB)\n",
      "Using cached scipy-1.9.3-cp310-cp310-macosx_12_0_arm64.whl (28.5 MB)\n",
      "Using cached starlette-0.21.0-py3-none-any.whl (64 kB)\n",
      "Using cached uvicorn-0.20.0-py3-none-any.whl (56 kB)\n",
      "Using cached watchdog-2.1.9-cp310-cp310-macosx_11_0_arm64.whl (88 kB)\n",
      "Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Using cached multidict-6.7.0-cp310-cp310-macosx_11_0_arm64.whl (44 kB)\n",
      "Using cached pydantic-1.10.26-cp310-cp310-macosx_11_0_arm64.whl (2.3 MB)\n",
      "Using cached yarl-1.22.0-cp310-cp310-macosx_11_0_arm64.whl (94 kB)\n",
      "Using cached ultralytics-8.3.241-py3-none-any.whl (1.1 MB)\n",
      "Using cached onnx-1.20.0-cp310-cp310-macosx_12_0_universal2.whl (18.3 MB)\n",
      "Using cached blobconverter-1.4.3-py3-none-any.whl (10 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached frozenlist-1.8.0-cp310-cp310-macosx_11_0_arm64.whl (49 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached imageio-2.37.2-py3-none-any.whl (317 kB)\n",
      "Using cached matplotlib-3.10.8-cp310-cp310-macosx_11_0_arm64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.2-cp310-cp310-macosx_11_0_arm64.whl (253 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.61.1-cp310-cp310-macosx_10_9_universal2.whl (2.9 MB)\n",
      "Using cached kiwisolver-1.4.9-cp310-cp310-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached ml_dtypes-0.5.4-cp310-cp310-macosx_10_9_universal2.whl (679 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached numba-0.63.1-cp310-cp310-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached llvmlite-0.46.0-cp310-cp310-macosx_11_0_arm64.whl (37.2 MB)\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl (37.3 MB)\n",
      "Using cached polars-1.36.1-py3-none-any.whl (802 kB)\n",
      "Using cached polars_runtime_32-1.36.1-cp39-abi3-macosx_11_0_arm64.whl (39.3 MB)\n",
      "Using cached propcache-0.4.1-cp310-cp310-macosx_11_0_arm64.whl (47 kB)\n",
      "Using cached protobuf-6.33.2-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "Using cached pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
      "Using cached pywavelets-1.8.0-cp310-cp310-macosx_11_0_arm64.whl (4.3 MB)\n",
      "Using cached pyyaml-6.0.3-cp310-cp310-macosx_11_0_arm64.whl (174 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tifffile-2025.5.10-py3-none-any.whl (226 kB)\n",
      "Using cached torch-2.9.1-cp310-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "Using cached fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached torchvision-0.24.1-cp310-cp310-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached filelock-3.20.1-py3-none-any.whl (16 kB)\n",
      "Using cached flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached markupsafe-3.0.3-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
      "Installing collected packages: mpmath, flatbuffers, filetype, appdirs, watchdog, urllib3, tqdm, sympy, sniffio, pyyaml, python-multipart, pyparsing, pydantic, protobuf, propcache, polars-runtime-32, pillow, numpy, networkx, multidict, MarkupSafe, llvmlite, kiwisolver, idna, humanfriendly, h11, fsspec, frozenlist, fonttools, filelock, cycler, click, charset-normalizer, certifi, attrs, async-timeout, yarl, uvicorn, tifffile, scipy, requests, PyWavelets, polars, opencv-python-headless, opencv-python, numba, ml_dtypes, jinja2, imageio, contourpy, coloredlogs, anyio, aiosignal, torch, starlette, scikit-image, pymatting, pooch, onnxruntime, onnx, matplotlib, imagehash, blobconverter, asyncer, aiohttp, ultralytics-thop, torchvision, fastapi, ultralytics, rembg\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70/70\u001b[0m [rembg]m68/70\u001b[0m [ultralytics]]]headless]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 PyWavelets-1.8.0 aiohttp-3.8.1 aiosignal-1.4.0 anyio-3.7.1 appdirs-1.4.4 async-timeout-4.0.3 asyncer-0.0.2 attrs-25.4.0 blobconverter-1.4.3 certifi-2025.11.12 charset-normalizer-2.1.1 click-8.1.3 coloredlogs-15.0.1 contourpy-1.3.2 cycler-0.12.1 fastapi-0.87.0 filelock-3.20.1 filetype-1.2.0 flatbuffers-25.12.19 fonttools-4.61.1 frozenlist-1.8.0 fsspec-2025.12.0 h11-0.16.0 humanfriendly-10.0 idna-3.11 imagehash-4.3.1 imageio-2.37.2 jinja2-3.1.6 kiwisolver-1.4.9 llvmlite-0.46.0 matplotlib-3.10.8 ml_dtypes-0.5.4 mpmath-1.3.0 multidict-6.7.0 networkx-3.4.2 numba-0.63.1 numpy-1.23.5 onnx-1.20.0 onnxruntime-1.13.1 opencv-python-4.11.0.86 opencv-python-headless-4.6.0.66 pillow-9.3.0 polars-1.36.1 polars-runtime-32-1.36.1 pooch-1.6.0 propcache-0.4.1 protobuf-6.33.2 pydantic-1.10.26 pymatting-1.1.8 pyparsing-3.3.1 python-multipart-0.0.5 pyyaml-6.0.3 rembg-2.0.28 requests-2.32.5 scikit-image-0.19.3 scipy-1.9.3 sniffio-1.3.1 starlette-0.21.0 sympy-1.14.0 tifffile-2025.5.10 torch-2.9.1 torchvision-0.24.1 tqdm-4.64.1 ultralytics-8.3.241 ultralytics-thop-2.0.18 urllib3-2.6.2 uvicorn-0.20.0 watchdog-2.1.9 yarl-1.22.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.10/site-packages (0.24.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.10/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.10/site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\n",
      "\n",
      "cuda machine: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "absoluteMVpath = os.getcwd()\n",
    "os.chdir(absoluteMVpath)\n",
    "import platform\n",
    "print(platform.system())\n",
    "isWindows = False\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    # if machine is windows\n",
    "    isWindows = True\n",
    "    print(\"windows user!\")\n",
    "else:\n",
    "    # if machine is NOT!!!! windows\n",
    "    isWindows = False\n",
    "    print (\"mac/linux user\")\n",
    "\n",
    "!git clone --progress --verbose https://github.com/ultralytics/ultralytics\n",
    "\n",
    "# we do not cd into ultralytics yet since we complete other object image tasks first\n",
    "# operations are inside ultralytics\n",
    "# create other folders that we will need later:\n",
    "\n",
    "# -p checks if the directory already exists before making it\n",
    "\n",
    "if isWindows:\n",
    "    isWindows = True\n",
    "    !mkdir -p .\\ultralytics\\objectImages\n",
    "    !mkdir -p .\\ultralytics\\ultralytics\\Dataset\\images\\test\n",
    "    !mkdir -p .\\ultralytics\\ultralytics\\Dataset\\images\\train\n",
    "    !mkdir -p .\\ultralytics\\ultralytics\\Dataset\\labels\\test\n",
    "    !mkdir -p .\\ultralytics\\ultralytics\\Dataset\\labels\\train\n",
    "else:\n",
    "    isWindows = False\n",
    "    !mkdir -p ./ultralytics/objectImages\n",
    "    !mkdir -p ./ultralytics/ultralytics/Dataset/images/test\n",
    "    !mkdir -p ./ultralytics/ultralytics/Dataset/images/train\n",
    "    !mkdir -p ./ultralytics/ultralytics/Dataset/labels/test\n",
    "    !mkdir -p ./ultralytics/ultralytics/Dataset/labels/train\n",
    "    \n",
    "os.chdir(absoluteMVpath)\n",
    "import subprocess\n",
    "isCuda = False\n",
    "\n",
    "# CHANGE URL BELOW:\n",
    "try:\n",
    "    subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "    # if machine is cuda\n",
    "    isCuda = True\n",
    "    %pip install \"ultralytics\" \"tqdm>=4.41.0\" \"pillow\" \"rembg==2.0.28\" \"onnx\" \"blobconverter\"\n",
    "    %pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n",
    "except:\n",
    "    # if machine is non-cuda\n",
    "    isCuda = False\n",
    "    %pip install \"ultralytics\" \"tqdm>=4.41.0\" \"pillow\" \"rembg==2.0.28\" \"onnx\" \"blobconverter\"\n",
    "    %pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "print(\"\\n\\n\\ncuda machine:\", isCuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Object Images\n",
    "\n",
    "Next we will clean up the data (images) we have by organizing/editing them\n",
    "\n",
    "Example: (note how there is no background and it is cropped right to the edge of the object)\n",
    "\n",
    "ORIGINAL:\n",
    "<img src=\"markdownimages/algae_6411.jpeg\" alt=\"unedited\" width=\"300\" height=\"390\">\n",
    "EDITED:\n",
    "<img src=\"markdownimages/editedimage.png\" alt=\"goodexample-editedimage\" width=\"300\" height=\"300\">\n",
    "\n",
    "---------\n",
    "\n",
    "#### BEFORE RUNNING THIS CODE CELL: Organize your images in folders within the original folder \n",
    "- Within `uneditedObjects`, make folders with the names of each gamepiece you are using and place each image in its respective directory.\n",
    "- **EVEN IF YOU HAVE ONLY ONE TYPE OF GAMEPIECE**, YOU MUST STILL PUT IT IN A FOLDER WITH ITS NAME WITHIN uneditedObjects!\n",
    "- example: if your object images are `algae` and `coral`, your directory should look something like this:\n",
    "```\n",
    "MonsterVision5\n",
    "├── uneditedObjects\n",
    "|     ├── algae\n",
    "|     |     └── images of algaes go here\n",
    "|     └── coral\n",
    "|           └── images of corals go here\n",
    "|\n",
    "└───── other folders we dont care about right now\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is NOT windows\n",
      "{}\n",
      "renaming images...\n",
      "removing background...\n",
      "0it [00:00, ?it/s]\n",
      "cropping images...\n",
      "images are now in /ultralytics/objectImages/!\n",
      "\n",
      "please confirm that EVERY image has been edited correctly\n",
      "\n",
      "\n",
      "zsh:1: no matches found: ./uneditedObjects/*\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "os.chdir(absoluteMVpath)\n",
    "\n",
    "if isWindows:\n",
    "    print(\"is windows\")\n",
    "    classNames = (os.listdir('.\\\\uneditedObjects\\\\')).sort()\n",
    "    CLASSES = {key: index for index, key in enumerate(classNames)} # DON'T EDIT\n",
    "    os.environ['CLASSES'] = json.dumps(CLASSES)\n",
    "    print(json.dumps(CLASSES))\n",
    "\n",
    "    print(\"renaming images...\")\n",
    "    # rename all images to fit the naming conventions\n",
    "    for object in os.listdir('.\\\\uneditedObjects\\\\'):\n",
    "        number = 0\n",
    "        for image in os.listdir('.\\\\uneditedObjects\\\\' + object):\n",
    "            os.rename('.\\\\uneditedObjects\\\\' + object + '\\\\' + image, '.\\\\uneditedObjects\\\\' + object + '_' + str(number) + '.png')\n",
    "            number += 1\n",
    "\n",
    "    # we first use rembg to remove background of images from our base images\n",
    "    # THEN we put them through Pillow to crop the images using the detected removed background\n",
    "    # then it saves it in ultralytics\n",
    "\n",
    "    print(\"removing background...\")\n",
    "    !mkdir .\\temporary\n",
    "    !rembg p .\\uneditedObjects temporary\n",
    "\n",
    "    print(\"cropping images...\")\n",
    "    for i in os.listdir('.\\\\temporary\\\\'):\n",
    "        img = Image.open(f\".\\\\temporary\\\\{i}\").convert(\"RGBA\") # MUST BE PNG FILE TO USE RGBA >:/\n",
    "        alpha = img.split()[3] # alpha channel is apparently the 4th channel in RGBA, it means transparency\n",
    "        # threshold makes sure that even if there are translucent pixels in the background we crop them\n",
    "        threshold = 10  # tweak this value if needed (0–255), 10 is pretty good for now\n",
    "        alpha = alpha.point(lambda p: p > threshold and 255)\n",
    "        # alpha.point is a function applied to all pixels in an image\n",
    "        # the threshold checks how translucent it is and either makes it fully transparent or opaque\n",
    "        img.putalpha(alpha) # recombine to an RGBA image\n",
    "        bbox = alpha.getbbox() # find the bounding box of non-transparent pixels (find the edge of where there actually is an image object)\n",
    "\n",
    "        if bbox:\n",
    "            cropped = img.crop(bbox)\n",
    "            cropped.save(f\".\\\\ultralytics\\\\objectImages\\\\{i}\") # the Final Destination (haha) of the images\n",
    "            \n",
    "    print(\"images are now in \\\\ultralytics\\\\objectImages\\\\!\\nplease confirm that EVERY image has been edited correctly\\n\\n\\n\\n\\n\\n\")\n",
    "    !del .\\temporary\n",
    "    !del .\\uneditedObjects\n",
    "else:\n",
    "    print(\"is NOT windows\")\n",
    "    classNames = os.listdir('./uneditedObjects/')\n",
    "    CLASSES = {key: index for index, key in enumerate(classNames)} # DON'T EDIT\n",
    "    os.environ['CLASSES'] = json.dumps(CLASSES)\n",
    "    print(json.dumps(CLASSES))\n",
    "\n",
    "    print(\"renaming images...\")\n",
    "    # rename all images to fit the naming conventions\n",
    "    for object in os.listdir('./uneditedObjects/'):\n",
    "        number = 0\n",
    "        for image in os.listdir('./uneditedObjects/' + object):\n",
    "            os.rename('./uneditedObjects/' + object + '/' + image, './uneditedObjects/' + object + '_' + str(number) + '.png')\n",
    "            number += 1\n",
    "\n",
    "    # we first use rembg to remove background of images from our base images\n",
    "    # THEN we put them through Pillow to crop the images using the detected removed background\n",
    "    # then it saves it in ultralytics\n",
    "\n",
    "    print(\"removing background...\")\n",
    "    !mkdir ./temporary/\n",
    "    !rembg p ./uneditedObjects temporary\n",
    "\n",
    "    print(\"cropping images...\")\n",
    "    for i in os.listdir('./temporary/'):\n",
    "        img = Image.open(f\"./temporary/{i}\").convert(\"RGBA\") # MUST BE PNG FILE TO USE RGBA >:/\n",
    "        alpha = img.split()[3] # alpha channel is apparently the 4th channel in RGBA, it means transparency\n",
    "        # threshold makes sure that even if there are translucent pixels in the background we crop them\n",
    "        threshold = 10  # tweak this value if needed (0–255), 10 is pretty good for now\n",
    "        alpha = alpha.point(lambda p: p > threshold and 255)\n",
    "        # alpha.point is a function applied to all pixels in an image\n",
    "        # the threshold checks how translucent it is and either makes it fully transparent or opaque\n",
    "        img.putalpha(alpha) # recombine to an RGBA image\n",
    "        bbox = alpha.getbbox() # find the bounding box of non-transparent pixels (find the edge of where there actually is an image object)\n",
    "\n",
    "        if bbox:\n",
    "            cropped = img.crop(bbox)\n",
    "            cropped.save(f\"./ultralytics/objectImages/{i}\") # the Final Destination (haha) of the images\n",
    "            \n",
    "    print(\"images are now in /ultralytics/objectImages/!\\n\\nplease confirm that EVERY image has been edited correctly\\n\\n\")\n",
    "    !rm -rf ./temporary/\n",
    "    !rm -rf ./uneditedObjects/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**before proceeding, please check the images to make sure they have been edited correctly (they are in `/ultralytics/objectImages/`)! If some images are not edited correctly, remove them or run through the process again**\n",
    "\n",
    "--------\n",
    "\n",
    "## 4. Download VOCDataset for Backing of Object Images\n",
    "We use the VOCDataset or Visualized Object Classes Dataset which is a dataset that contains many images with labels for training of pascal. We are just extracting the images from a few of the datasets for backing images for our yolo training images.\n",
    "\n",
    "Download the 2007 and 2012 VOCDataset and put them in a separate directory (may take 10+ minutes depending on wifi) and extract the downloaded .tar archive files (should create folders `VOC2007` and `VOC2012` in `VOCdevkit`. If not then run again):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0--:-- --:--:--     0\n",
      "  0 3605M    0 3599k    0     0  3628k      0  0:16:57 --:--:--  0:16:57 8089k\n",
      "curl: (56) Failure writing output to destination, passed 16375 returned 0\n",
      "Archive:  ./VOCdevkit/pascal-voc-2012-dataset\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of ./VOCdevkit/pascal-voc-2012-dataset or\n",
      "        ./VOCdevkit/pascal-voc-2012-dataset.zip, and cannot find ./VOCdevkit/pascal-voc-2012-dataset.ZIP, period.\n"
     ]
    }
   ],
   "source": [
    "os.chdir(absoluteMVpath + \"/ultralytics\")\n",
    "if isWindows:\n",
    "    isWindows = True\n",
    "    !mkdir -p .\\VOCdevkit\\test\n",
    "    !mkdir -p .\\VOCdevkit\\train\n",
    "    !curl -L -o .\\VOCdevkit\\pascal-voc-2012-dataset.zip https://www.kaggle.com/api/v1/datasets/download/gopalbhattrai/pascal-voc-2012-dataset\n",
    "    !tar -xvf .\\VOCdevkit\\pascal-voc-2012-dataset.zip -C .\\VOCdevkit\\\n",
    "    print(\"got VOCdevkit\")\n",
    "else:\n",
    "    isWindows = False\n",
    "    !mkdir -p ./VOCdevkit/test\n",
    "    !mkdir -p ./VOCdevkit/train\n",
    "    !curl -L -o ./VOCdevkit/pascal-voc-2012-dataset.zip https://www.kaggle.com/api/v1/datasets/download/gopalbhattrai/pascal-voc-2012-dataset\n",
    "    !unzip ./VOCdevkit/pascal-voc-2012-dataset -d ./VOCdevkit/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 27 3605M   27  986M    0     0  42.7M      0  0:01:24  0:00:23  0:01:01 47.1M02:12  0:00:05  0:02:07 32.0M0  41.8M      0  0:01:26  0:00:18  0:01:08 46.8M\n",
      "curl: (56) Failure writing output to destination, passed 16366 returned 135\n",
      "Archive:  ./VOCdevkit/pascal-voc-2012-dataset\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of ./VOCdevkit/pascal-voc-2012-dataset or\n",
      "        ./VOCdevkit/pascal-voc-2012-dataset.zip, and cannot find ./VOCdevkit/pascal-voc-2012-dataset.ZIP, period.\n"
     ]
    }
   ],
   "source": [
    "os.chdir(absoluteMVpath + \"/ultralytics\")\n",
    "!mkdir -p ./VOCdevkit \n",
    "!mkdir -p ./VOCdevkit/test\n",
    "!mkdir -p ./VOCdevkit/train\n",
    "# -p (lowercase) checks if the directory already exists before making\n",
    "# the curl command is pre-installed on Mac\n",
    "!curl -L -o ./pascal-voc-2012-dataset https://www.kaggle.com/api/v1/datasets/download/gopalbhattrai/pascal-voc-2012-dataset\n",
    "\n",
    "!unzip ./VOCdevkit/pascal-voc-2012-dataset -d ./VOCdevkit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(absoluteMVpath + \"/ultralytics/VOCdevkit\")\n",
    "for i in os.listdir('./VOC2012_test/VOC2012_test/JPEGImages'): # listdir wil return a list of strings of every file and folder name in the DIR\n",
    "    os.rename('./VOC2012_test/VOC2012_test/JPEGImages/' + i, './test/' + i) # takes original path and appends each image file then moves it by \"renaming\" it\n",
    "for i in os.listdir('./VOC2012_train_val/VOC2012_train_val/JPEGImages'):\n",
    "    os.rename('./VOC2012_train_val/VOC2012_train_val/JPEGImages/' + i, './train/' + i)\n",
    "\n",
    "os.chdir(absoluteMVpath + \"/ultralytics/VOCdevkit\")\n",
    "if isWindows:\n",
    "    isWindows = True\n",
    "    !rmdir ./VOC2012_test/\n",
    "    !rmdir ./VOC2012_train_val/\n",
    "    !del pascal-voc-2012-dataset\n",
    "else:\n",
    "    isWindows = False\n",
    "    !rm -rf ./VOC2012_test/\n",
    "    !rm -rf ./VOC2012_train_val/\n",
    "    !rm pascal-voc-2012-dataset\n",
    "\n",
    "os.chdir(absoluteMVpath + \"/ultralytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Overlay objects on backing images and generate YOLO labels\n",
    "\n",
    "This script automatically generates synthetic training data for object detection models (such as YOLO) by compositing foreground object images onto random background images.\n",
    "\n",
    "### Overview:\n",
    "1. Randomly selects an object image (with transparent background) and a random background image.\n",
    "2. Scales the object to a random percentage of the background’s size.\n",
    "3. Pastes the scaled object onto a random position within the background.\n",
    "4. Saves the resulting composite image to the dataset folder (e.g., ./ultralytics/Dataset/images/...).\n",
    "5. Generates a corresponding YOLO-format label (.txt) containing the object’s class ID and bounding box coordinates\n",
    "   (normalized x_center, y_center, width, height).\n",
    "6. Designed for multiprocessing to efficiently generate large datasets.\n",
    "\n",
    "### Functions:\n",
    "- stackAndScaleImage(): Scales and pastes one image onto another using PIL.\n",
    "- selectScaleAndCreateYoloLabels(): Combines object and background, computes YOLO label data.\n",
    "- combineRandomImages(): Picks random images from directories and combines them.\n",
    "- makeImage(): Creates and saves a single labeled synthetic training image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(absoluteMVpath + \"/ultralytics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dataGen.py \n",
    "# we write to a new file so we can do multiprocessing on a separate python file (we cant do multiprocessing in .ipynb)\n",
    "from PIL import Image\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm as progressBar\n",
    "import threading\n",
    "import multiprocessing\n",
    "import concurrent.futures\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "def stackAndScaleImage(objectImage, backgroundImage, scalePercent, position): # Takes a first PNG image, scales it down a certain percentage and pastes it on a second PNG image.\n",
    "\n",
    "# objectImage: PIL image of object\n",
    "# backgroundImage: PIL image of background\n",
    "# scalePercent: Percentage to scale down the first image. (MAKE FLOAT INSTEAD OF DUMB STUFF)\n",
    "# position: Tuple of (x, y) coordinates to paste the scaled image.\n",
    "\n",
    "  objectImage = Image.open(objectImage)\n",
    "  backgroundImage = Image.open(backgroundImage)\n",
    "\n",
    "  # Scale down the first image\n",
    "  width, height = objectImage.size # extract the width and height of object\n",
    "  newWidth = int(width * scalePercent) # create a new width for object based on the scalePercent and makes it an int\n",
    "  newHeight = int(height * scalePercent) # create a new height for object based on the scalePercent and makes it an int\n",
    "  objectScaled = objectImage.resize((newWidth, newHeight)) # use the resize method of a PIL Image to scale object to the new_width and new_height\n",
    " \n",
    "  # Scaled the hight to add more variation to the data\n",
    "  width, height = objectScaled.size # Reset the width and height variables to be the new width and height of object after scaling\n",
    "  randomHeightScale = random.uniform(0.8, 1.2) # Choose a random scalePercent between the minimum and maximum values.\n",
    "  # randomHeightScale = 1.0\n",
    "  newHeight = int(height * randomHeightScale) # Calculate the newHeight based on the random scalePercent above\n",
    "  if newHeight > backgroundImage.height:\n",
    "    newHeight = backgroundImage.height\n",
    "    print(\"triggered\", newHeight)\n",
    "  objectScaled = objectScaled.resize((width, newHeight)) # Resize the image just like above\n",
    "  \n",
    "  backgroundImage.paste(objectScaled, position, objectScaled) # Second parameter makes it so that the pixels with no value meant to be clear stay clear and aren't black\n",
    "\n",
    "  return (backgroundImage, newHeight) # Return the final stacked image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SCALE PERCENTAGE IS HOW MUCH OF THE FRAME YOU WANT TO TAKE UP NOT HOW MUCH YOU WANT TO SCALE THE OBJECT IMAGE DOWN\n",
    "def selectScaleAndCreateYoloLabels(objectPath, backgroundPath, minSizePercent, maxSizePercent, objectClassStr, CLASSES): \n",
    "  # Combines two images by pasting a scaled version of an object onto a background. SCALES BASED ON WIDTH\n",
    "\n",
    "  # objectPath: Path to the object image.\n",
    "  # backgroundPath: Path to the background image.\n",
    "  # minSizePercent: Minimum percentage to scale down object's WIDTH.\n",
    "  # maxSizePercent: Maximum percentage to scale down object's WIDTH.\n",
    "\n",
    "  object = Image.open(objectPath) # Open the first image as object\n",
    "  background = Image.open(backgroundPath) # Open the second image as background\n",
    "\n",
    "  objectWidth, objectHeight = object.size # Extract the width and height of object as objectWidth and objectHeight\n",
    "  backgroundWidth, backgroundHeight = background.size # Extract the width and height of background as backgroundWidth and backgroundHeight\n",
    "\n",
    "  # Choose a random scalePercent between the minimum and maximum values provided as parameters\n",
    "  # NEVER ABOVE 1.0(?)\n",
    "  scalePercent = random.uniform(minSizePercent, maxSizePercent)\n",
    "\n",
    "  # as background's width before applying this random scale percent (should really be same as shortest side?)\n",
    "  # baseScale = backgroundWidth / objectWidth # The baseScale is the ratio of how big background's width is compared to object's width (ONLY WITH MEASUREMENTS)\n",
    "  if backgroundWidth <= backgroundHeight: # scale based on shortest side of background\n",
    "    baseScale = backgroundWidth/objectWidth\n",
    "    used = \"width shortest so base scale is based on width\"\n",
    "  else:\n",
    "    baseScale = backgroundHeight/objectHeight\n",
    "    used = \"height shortest so base scale is based on height\"\n",
    "\n",
    "  # print({\n",
    "  #   \"backgroundWidth\": backgroundWidth,\n",
    "  #   \"object.width\": object.width,\n",
    "  #   \"objectWidth\": objectWidth,\n",
    "  #   \"baseScale\": baseScale,\n",
    "  #   \"scalePercent\": scalePercent,\n",
    "  #   \"baseScale * scalePercent\": baseScale * scalePercent,\n",
    "  #   \"object.width * scalePercent/100\": object.width * scalePercent/100,\n",
    "  #   \"object.height * scalePercent/100\": object.height * scalePercent/100,\n",
    "  # })\n",
    "\n",
    "\n",
    "  scalePercent = baseScale * scalePercent # fix the scalePercent to include the baseScale between the 2 images and be proportional\n",
    "\n",
    "  # Choose a random position for object in background\n",
    "  widthOfObjectAfterScaling = int(object.width * scalePercent)   # predict width of object after scaling so you can choose a random width in bounds of background\n",
    "  heightOfObjectAfterScaling = int(object.height * scalePercent) # predict height of object after scaling so you can choose a random height in bounds of background\n",
    "  x = random.randint(0, backgroundWidth - widthOfObjectAfterScaling)     # select random x position for object in background\n",
    "\n",
    "  # ERROR IS HERE: The error is because background isn't tall enough to fit object even after scaling so the randint is trying to selced from 0 to a negative number\n",
    "  y = random.randint(0, backgroundHeight - heightOfObjectAfterScaling)   # select random y position for object in background\n",
    "\n",
    "  (combinedImage, finalHeight) = stackAndScaleImage(objectPath, backgroundPath, scalePercent, (x, y)) # Use the stack_scaled_images function to combine the two images\n",
    "\n",
    "  # finalWidth, finalHeight = combinedImage.size # WRONG\n",
    "\n",
    "  # Save the paste_parameters as a json object\n",
    "  debugParameters = {\n",
    "    \"width_background\": backgroundWidth,\n",
    "    \"height_background\": backgroundHeight,\n",
    "    \"paste_x\": x,\n",
    "    \"paste_y\": y,\n",
    "    \"paste_width\": widthOfObjectAfterScaling,\n",
    "    \"paste_height\": finalHeight,\n",
    "    \"scale_type\": used\n",
    "  }\n",
    "  # The position for object to be pasted onto background is randomly selected within the bounds of background\n",
    "\n",
    "  # https://docs.cogniflow.ai/en/article/how-to-create-a-dataset-for-object-detection-using-the-yolo-labeling-format-1tahk19/\n",
    "  # YOLO labeling parameters\n",
    "  pasteParametersYolo = {\n",
    "    \"objectClassNum\": CLASSES[objectClassStr],\n",
    "    \"x_center\": (x + widthOfObjectAfterScaling/2.0) / backgroundWidth, # calculate what percentage of the width of background the center of scaled object will be\n",
    "    \"y_center\": (y + finalHeight/2.0) / backgroundHeight, # calculate what percentage of the height of background the center of scaled object will be\n",
    "    \"width\": widthOfObjectAfterScaling / backgroundWidth, # calculate what percentage of the width of background does scaled object take\n",
    "    \"height\": finalHeight / backgroundHeight, # calculate what percentage of the height of background does scaled object take\n",
    "  }\n",
    "\n",
    "  return (combinedImage, debugParameters, pasteParametersYolo) # return the PIL image object after stacking, the parameters used for pasting, and the parameters used for pasting in a YOLO suitable notation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def combineRandomImages(directory1, directory2, minSizePercent, maxSizePercent, CLASSES):\n",
    "  # Combines two random images from the two directories using the first function, returns a PIL Image object with the combined images\n",
    "\n",
    "  #   directory1: Path to the first directory.\n",
    "  #   directory2: Path to the second directory.\n",
    "  #   minSizePercent: Minimum percentage to scale down the first image.\n",
    "  #   maxSizePercent: Maximum percentage to scale down the first image.\n",
    "\n",
    "  # Get all files in directories. If one isn't an image it could break\n",
    "  objectFiles = os.listdir(directory1) # Get a list of all files in the first directory\n",
    "  backgroundFiles = os.listdir(directory2) # Get a list of all files in the second directory\n",
    "\n",
    "  imageChosen = random.choice(objectFiles)\n",
    "  objectClass = imageChosen.split(\"_\")[0]\n",
    "\n",
    "  # Choose random images\n",
    "  objectPath = os.path.join(directory1, imageChosen) # Choose a random file from the first directory\n",
    "  backgroundPath = os.path.join(directory2, random.choice(backgroundFiles)) # Choose a random file from the second directory\n",
    "\n",
    "  # Use the combine_images_james_xy function to combine the two images.\n",
    "  combinedImage, debugParameters, pasteParametersYolo = selectScaleAndCreateYoloLabels(objectPath, backgroundPath, minSizePercent, maxSizePercent, objectClass, CLASSES)\n",
    "\n",
    "  # print(pasteParametersYolo)\n",
    "\n",
    "  return (combinedImage, debugParameters, pasteParametersYolo) # re-return all of the returns from the combine_images_james_xy function\n",
    "\n",
    "# def makeImage(testOrTrain=\"test\", numImages=10, minSizePercent=.05, maxSizePercent=.8, i=-1):\n",
    "def makeImage(args):\n",
    "  (testOrTrain, numImages, minSizePercent, maxSizePercent, CLASSES, i) = args\n",
    "  if i == -1:\n",
    "    raise ValueError(\"Invalid i Value\")\n",
    "  # Combine the images from directory1 (game object) with images from directory2 (backgrounds).\n",
    "  combinedImage, debugParameters, pasteParametersYolo = combineRandomImages(\"./objectImages\", \"./VOCdevkit/\"+testOrTrain, minSizePercent, maxSizePercent, CLASSES)\n",
    "  # Figure out a file name based on the current iteration and type of dataset\n",
    "  baseFilename = f\"{testOrTrain}_{i:0{6}d}\" # Max 100000 file names\n",
    "  # Save the image to the specified folder based on type of data set and use the above created filename\n",
    "  combinedImage.save('./ultralytics/Dataset/images/'+testOrTrain+'/'+baseFilename+'.png')\n",
    "\n",
    "  # Open/create a text file with the same name as the image and add the paste_parameters_yolo to it\n",
    "  with open('./ultralytics/Dataset/labels/'+testOrTrain+'/'+baseFilename+'.txt', \"w\") as f:\n",
    "    yoloData = pasteParametersYolo\n",
    "    if round(yoloData['x_center'],6) > 1 or round(yoloData['y_center'],6) > 1:\n",
    "      print(\"ERROR at\", i)\n",
    "      print(yoloData)\n",
    "      print(debugParameters)\n",
    "    f.write(f\"{yoloData['objectClassNum']} {round(yoloData['x_center'],6)} {round(yoloData['y_center'],6)} {round(yoloData['width'],6)} {round(yoloData['height'],6)}\") # write data and round it to the correct decimal places (first digit in string is the class)\n",
    "\n",
    "\n",
    "\n",
    "# Main function that ties together all of the other functions to make it work\n",
    "def makeData(testOrTrain=\"test\", numImages=10, minSizePercent=.05, maxSizePercent=.8):\n",
    "  args = tuple([(testOrTrain, numImages, minSizePercent, maxSizePercent, CLASSES, i) for i in range(numImages)]) # creates a tuple of tuple\n",
    "\n",
    "  with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    executor.map(makeImage, args)\n",
    "    print(\"yippie!!! it works\") # to make sure it works :)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\"--classes\", type=str)\n",
    "  parser.add_argument(\"--testOrTrain\", default=\"test\", type=str)\n",
    "  parser.add_argument(\"--numImages\", default=2, type=int)\n",
    "  parser.add_argument(\"--minSizePercent\", default=.05, type=float)\n",
    "  parser.add_argument(\"--maxSizePercent\", default=.8, type=float)\n",
    "\n",
    "  inputArgs = parser.parse_args()\n",
    "\n",
    "  CLASSES = json.loads(inputArgs.classes.replace(\"'\", '\"'))\n",
    "  testOrTrain = inputArgs.testOrTrain\n",
    "  numImages = inputArgs.numImages\n",
    "  minSizePercent = inputArgs.minSizePercent\n",
    "  maxSizePercent = inputArgs.maxSizePercent\n",
    "\n",
    "  testOrTrain = testOrTrain.lower()\n",
    "\n",
    "  makeData(testOrTrain, numImages, minSizePercent, maxSizePercent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the test and train image sets. You should pick a size percent that makes sense for the application. Here are the lines to make a test and train dataset:\n",
    "\n",
    "(This is going to take a few minutes on some machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train code\n",
    "os.chdir(absoluteMVpath + \"/ultralytics\")\n",
    "!python3 dataGen.py --classes=\"$CLASSES\" --testOrTrain=\"train\" --numImages=15000 --minSizePercent=.1 --maxSizePercent=.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "os.chdir(absoluteMVpath + \"/ultralytics\")\n",
    "!python3 dataGen.py --classes=\"$CLASSES\" --testOrTrain=\"test\" --numImages=7500 --minSizePercent=.1 --maxSizePercent=.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the dataset config file for training yolo but **make sure to enumerate the classes at the bottom and pick names that are appropriate**:\n",
    "\n",
    "(USE THE NAMES LISTED IN CLASSNAMES) AND PUT THEM IN THE SAME ORDER AS IN THE LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classNames)\n",
    "os.chdir(absoluteMVpath + \"/ultralytics/ultralytics/cfg/datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile CUSTOM.yaml\n",
    "# Ultralytics YOLO 🚀, AGPL-3.0 license\n",
    "# Documentation: # Documentation: https://docs.ultralytics.com/datasets/detect/voc/\n",
    "# Example usage: yolo train data=CUSTOM.yaml\n",
    "# parent\n",
    "# ├── ultralytics\n",
    "# └── datasets\n",
    "#     └── CUSTOM\n",
    "\n",
    "# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\n",
    "path: ./ultralytics/Dataset/\n",
    "train: images/train # train images (relative to 'path')  16551 images\n",
    "val: images/test # val images (relative to 'path')  4952 images\n",
    "test: images/test # test images (optional)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set the number of classes\n",
    "nc: 4 # EDIT TO REFLECT CORRECT AMOUNT OF CLASSES\n",
    "\n",
    "# Classes\n",
    "names: # EDIT HERE\n",
    "  0: coral\n",
    "  1: note\n",
    "  2: algae\n",
    "  3: carrot\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model\n",
    "\n",
    "Now that we have the dataset we can train the model. First we will run the checks command to make sure everything is good (it has all the libraries, the computer is able to train it, etc).\n",
    "We are going to use the nano version of yolov11 because it is the smallest, fastest, and most lightweight version. We will specify the name of the custom data config file that we created above as well as yolov11n.pt for the nano version of the pretrained model. I've decided on 10 epochs for just a short retraining of the model. The imgsz is 640 which will resize all of the images we created to 640x640 for input into the model. This number can be changed for speed over accuracy if needed. I set the batch size to use 90% of the available GPU memory as well as set cache to true to improve speed.\n",
    "\n",
    "--------\n",
    "\n",
    "my face when yolov12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo checks\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "!yolo help\n",
    "!yolo settings datasets_dir=ultralytics\n",
    "# i am a genuous....\n",
    "from ultralytics import settings\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imgsz is set to 512 because it must be a multiple of 32 and not greater than 530 to work with the camera with the OV... sensor\n",
    "\n",
    "KEEP IN MIND:\n",
    "if you didn't already know, training takes a while (as in I started training at around 1 am. it is now 8 am). Depending on what system you're on, it will probably take less long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(absoluteMVpath)\n",
    "if isCuda:\n",
    "    isCuda = True\n",
    "    !yolo detect train data=\"./ultralytics/ultralytics/cfg/datasets/CUSTOM.yaml\" model=\"yolo12n.pt\" epochs=2 imgsz=512 name=\"lakeMONSTER\" batch=0.5 patience=2 cache=disk workers=8 device=0 project=./datagenTraining/\n",
    "else:\n",
    "    isCuda = False\n",
    "    !yolo detect train data=\"./ultralytics/ultralytics/cfg/datasets/CUSTOM.yaml\" model=\"yolo12n.pt\" epochs=2 imgsz=512 name=\"lakeMONSTER\" batch=0.5 patience=2 cache=disk workers=8 device=cpu project=./datagenTraining/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Upload to Luxonis to convert to OpenVINO format\n",
    "Run the below code to get your .blob to be used in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blobconverter\n",
    "\n",
    "!yolo export model=\"best.pt\" format=onnx opset=13 simplify\n",
    "\n",
    "blobconverter.from_onnx(model=\"yolo12n.onnx\", shaves=6, data_type=\"FP16\", output_dir=\"./finalProduct/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## End\n",
    "You are done with this notebook! The next steps of using the models come in after you set up MonsterVision on the robot. Refer to the README.md for more clear instructions on when the models come in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
