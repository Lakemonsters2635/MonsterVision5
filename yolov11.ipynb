{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Data Generation for YOLOv7+ with VOCDataset and Training of YOLOv12\n",
    "1. Take pictures\n",
    "1. Prepare object images\n",
    "1. Prepare environment\n",
    "1. Organize the object images\n",
    "3. Download VOCDataset for backing of object images\n",
    "1. Overlay objects on backing images and generate YOLO labels\n",
    "9000. Train model\n",
    "1. Upload to Luxonis to convert to OpenVINO format\n",
    "1. Export to Raspberry Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- test on Windows\n",
    "- write program for editing and cropping object images (remove background: rembg 2.0.28) (crop images: https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.crop)\n",
    "- test YOLO v12\n",
    "- there are multiple errors/issues in 6 apparently (in each function!)\n",
    "- same for 4 i guess...\n",
    "- make sure 8 is correct\n",
    "- idk how to do 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Take Pictures\n",
    "\n",
    "When taking pictures of objects:\n",
    "- TAKE MANY PHOTOS AT ALL POSSIBLE ANGLES (30-ish photos depending on how many angles we need)\n",
    "- Good lighting and quality, of course\n",
    "- Try to have the object be the only thing in frame (no extra objects that could possibly take away focus from the image)\n",
    "- The object should be completely in frame and not cut off\n",
    "\n",
    "Good example:\n",
    "\n",
    "<img src=\"YOLO/markdownimages/baseimage.png\" alt=\"goodexample-baseimage\" width=\"300\" height=\"300\">\n",
    "\n",
    "\n",
    "## 2. Prepare Object Images\n",
    "Once you have your images head over to [remove.bg](https://remove.bg) and upload each image. Then open up each image without a background to GIMP or your photo editor of choice and crop the images until you are right at the edge of them down to the pixel. \n",
    "THIS IS TO BE REPLACED BY WHATEVER PROGRAM JULIA IS WRITING IDK (this file will do the editing process for you!!!)\n",
    "\n",
    "Good example: (note how there is no background and it is cropped right to the edge of the object)\n",
    "\n",
    "<img src=\"YOLO/markdownimages/editedimage.png\" alt=\"goodexample-editedimage\" width=\"300\" height=\"300\">\n",
    "\n",
    "\n",
    "## 3. Prepare Environment\n",
    "\n",
    "Run the below commands to make a virtual environment and clone the ultralytics github which contains yolo, then cd into ultralytics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv .venv\n",
    "# might have to be !python3.10, or just !python3 \n",
    "# depends on what the machine already has installed\n",
    "!source .venv/bin/activate\n",
    "%cd YOLO/\n",
    "!git clone --progress --verbose https://github.com/ultralytics/ultralytics\n",
    "%cd ultralytics/\n",
    "\n",
    "# change directory into ultralytics so subsequent cmd line \n",
    "# operations are inside ultralytics\n",
    "# create other folders that we will need later:\n",
    "\n",
    "!mkdir ./objectImages\n",
    "!mkdir ./ultralytics/Dataset\n",
    "!mkdir ./ultralytics/Dataset/images\n",
    "!mkdir ./ultralytics/Dataset/labels\n",
    "!mkdir ./ultralytics/Dataset/images/test\n",
    "!mkdir ./ultralytics/Dataset/images/train\n",
    "!mkdir ./ultralytics/Dataset/labels/test\n",
    "!mkdir ./ultralytics/Dataset/labels/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CUDA version if you have CUDA for pytorch installation (if you don't have CUDA then skip down to the second pip install segment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit [pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/) to install the correct version of pytorch (only change the url part of the command and if you have a newer version of CUDA than pytorch has then install the latest pytorch version). This could take a while depending on your network speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for yolov11 and all other ultralytics yolo versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics tqdm>=4.41.0 pillow\n",
    "# if the above command returns \"zsh:1: 4.41.0 not found\" then run the below command:\n",
    "# %pip install \"ultralytics\" \"tqdm>=4.41.0\" \"pillow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Organize Object Images\n",
    "Currently the file structure of important files should look like:\n",
    "```\n",
    "ultralytics\n",
    "â”œâ”€â”€ ultralytics\n",
    "|     â”œâ”€â”€ Dataset\n",
    "|     |     â”œâ”€â”€ images\n",
    "|     |     â””â”€â”€ labels\n",
    "|     â””â”€â”€ (folders with all of the yolo code and other things we will get to later)\n",
    "â”œâ”€â”€ objectImages\n",
    "â””â”€â”€ (more yolo files and folders)\n",
    "```\n",
    "\n",
    "Rename all of the images to `*class*_*number*`<br>\n",
    "ex: `blue_1`, `cone_29`, `banana_0`\n",
    "\n",
    "#### Take all of the images you just edited and put them in the folder named `objectImages`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now edit the list below to contain all of your class names. We will need this later for generating the dataset and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classNames = [\"algae\"] # Edit\n",
    "import os\n",
    "import json\n",
    "CLASSES = {key: index for index, key in enumerate(classNames)} # DON'T EDIT\n",
    "os.environ['CLASSES'] = json.dumps(CLASSES)\n",
    "print(json.dumps(CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download VOCDataset for Backing of Object Images\n",
    "We use the VOCDataset or Visualized Object Classes Dataset which is a dataset that contains many images with labels for training of pascal. We are just extracting the images from a few of the datasets for backing images for our yolo training images.\n",
    "\n",
    "Download the 2007 and 2012 VOCDataset and put them in a separate directory (may take 10+ minutes depending on wifi) and extract the downloaded .tar archive files (should create folders `VOC2007` and `VOC2012` in `VOCdevkit`. If not then run again):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINUX/MAC\n",
    "\n",
    "!mkdir -p ./VOCdevkit \n",
    "# -p (lowercase) checks if the directory already exists before making\n",
    "# !wget -P ./VOCdevkit https://www.kaggle.com/api/v1/datasets/download/gopalbhattrai/pascal-voc-2012-dataset\n",
    "\n",
    "# if running on Mac, you will only be able to use !wget if you have it installed already.\n",
    "# the curl command is pre-installed on Mac\n",
    "!curl -L -o ./VOCdevkit/pascal-voc-2012-dataset https://www.kaggle.com/api/v1/datasets/download/gopalbhattrai/pascal-voc-2012-dataset\n",
    "\n",
    "!unzip ./VOCdevkit/pascal-voc-2012-dataset -d ./VOCdevkit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOWS\n",
    "\n",
    "!mkdir -p ./VOCdevkit\n",
    "# -p checks if the directory already exists before making\n",
    "!curl -L -o ./VOCdevkit/pascal-voc-2012-dataset https://www.kaggle.com/api/v1/datasets/download/gopalbhattrai/pascal-voc-2012-dataset\n",
    "!tar -xvf ./VOCdevkit/pascal-voc-2012-dataset.zip -C ./VOCdevkit/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert VOC dataset to YOLO-format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for i in os.listdir('./VOCdevkit/VOC2012_test/VOC2012_test/JPEGImages'): # listdir wil return a list of strings of every file and folder name in the DIR\n",
    "    os.rename('./VOCdevkit/VOC2012_test/VOC2012_test/JPEGImages/' + i, './ultralytics/Dataset/images/test/' + i) # takes original path and appends each image file then moves it by \"renaming\" it\n",
    "for i in os.listdir('./VOCdevkit/VOC2012_train_val/VOC2012_train_val/JPEGImages'):\n",
    "    os.rename('./VOCdevkit/VOC2012_train_val/VOC2012_train_val/JPEGImages/' + i, './ultralytics/Dataset/images/train/' + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, clean up your files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINUX/MAC\n",
    "\n",
    "!rm ./VOCdevkit/pascal-voc-2012-dataset\n",
    "!rm -rf ./VOCdevkit/VOC2012_test/\n",
    "!rm -rf ./VOCdevkit/VOC2012_train_val/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOWS\n",
    "\n",
    "!del ./VOCdevkit/pascal-voc-2012-dataset\n",
    "!rmdir ./VOCdevkit/VOC2012_test/\n",
    "!rmdir ./VOCdevkit/VOC2012_train_val/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "e\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overlay objects on backing images and generate YOLO labels\n",
    "\n",
    "This script automatically generates synthetic training data for object detection models (such as YOLO) by compositing foreground object images onto random background images.\n",
    "\n",
    "### Overview:\n",
    "1. Randomly selects an object image (with transparent background) and a random background image.\n",
    "2. Scales the object to a random percentage of the backgroundâ€™s size.\n",
    "3. Pastes the scaled object onto a random position within the background.\n",
    "4. Saves the resulting composite image to the dataset folder (e.g., ./ultralytics/Dataset/images/...).\n",
    "5. Generates a corresponding YOLO-format label (.txt) containing the objectâ€™s class ID and bounding box coordinates\n",
    "   (normalized x_center, y_center, width, height).\n",
    "6. Designed for multiprocessing to efficiently generate large datasets.\n",
    "\n",
    "### Functions:\n",
    "- stackAndScaleImage(): Scales and pastes one image onto another using PIL.\n",
    "- selectScaleAndCreateYoloLabels(): Combines object and background, computes YOLO label data.\n",
    "- combineRandomImages(): Picks random images from directories and combines them.\n",
    "- makeImage(): Creates and saves a single labeled synthetic training image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./dataGen.py \n",
    "# we write to a new file so we can do multiprocessing on a separate python file (we cant do multiprocessing in .ipynb)\n",
    "from PIL import Image\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm as progressBar\n",
    "import threading\n",
    "import multiprocessing\n",
    "import concurrent.futures\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "def stackAndScaleImage(objectImage, backgroundImage, scalePercent, position): # Takes a first PNG image, scales it down a certain percentage and pastes it on a second PNG image.\n",
    "\n",
    "# objectImage: PIL image of object\n",
    "# backgroundImage: PIL image of background\n",
    "# scalePercent: Percentage to scale down the first image. (MAKE FLOAT INSTEAD OF DUMB STUFF)\n",
    "# position: Tuple of (x, y) coordinates to paste the scaled image.\n",
    "\n",
    "  objectImage = Image.open(objectImage)\n",
    "  backgroundImage = Image.open(backgroundImage)\n",
    "\n",
    "  # Scale down the first image\n",
    "  width, height = objectImage.size # extract the width and height of object\n",
    "  newWidth = int(width * scalePercent) # create a new width for object based on the scalePercent and makes it an int\n",
    "  newHeight = int(height * scalePercent) # create a new height for object based on the scalePercent and makes it an int\n",
    "  objectScaled = objectImage.resize((newWidth, newHeight)) # use the resize method of a PIL Image to scale object to the new_width and new_height\n",
    " \n",
    "  # Scaled the hight to add more variation to the data\n",
    "  width, height = objectScaled.size # Reset the width and height variables to be the new width and height of object after scaling\n",
    "  randomHeightScale = random.uniform(0.8, 1.2) # Choose a random scalePercent between the minimum and maximum values.\n",
    "  # randomHeightScale = 1.0\n",
    "  newHeight = int(height * randomHeightScale) # Calculate the newHeight based on the random scalePercent above\n",
    "  if newHeight > backgroundImage.height:\n",
    "    newHeight = backgroundImage.height\n",
    "    print(\"triggered\", newHeight)\n",
    "  objectScaled = objectScaled.resize((width, newHeight)) # Resize the image just like above\n",
    "  \n",
    "  backgroundImage.paste(objectScaled, position, objectScaled) # Second parameter makes it so that the pixels with no value meant to be clear stay clear and aren't black\n",
    "\n",
    "  return (backgroundImage, newHeight) # Return the final stacked image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SCALE PERCENTAGE IS HOW MUCH OF THE FRAME YOU WANT TO TAKE UP NOT HOW MUCH YOU WANT TO SCALE THE OBJECT IMAGE DOWN\n",
    "def selectScaleAndCreateYoloLabels(objectPath, backgroundPath, minSizePercent, maxSizePercent, objectClassStr, CLASSES): \n",
    "  # Combines two images by pasting a scaled version of an object onto a background. SCALES BASED ON WIDTH\n",
    "\n",
    "  # objectPath: Path to the object image.\n",
    "  # backgroundPath: Path to the background image.\n",
    "  # minSizePercent: Minimum percentage to scale down object's WIDTH.\n",
    "  # maxSizePercent: Maximum percentage to scale down object's WIDTH.\n",
    "\n",
    "  object = Image.open(objectPath) # Open the first image as object\n",
    "  background = Image.open(backgroundPath) # Open the second image as background\n",
    "\n",
    "  objectWidth, objectHeight = object.size # Extract the width and height of object as objectWidth and objectHeight\n",
    "  backgroundWidth, backgroundHeight = background.size # Extract the width and height of background as backgroundWidth and backgroundHeight\n",
    "\n",
    "  # Choose a random scalePercent between the minimum and maximum values provided as parameters\n",
    "  # NEVER ABOVE 1.0(?)\n",
    "  scalePercent = random.uniform(minSizePercent, maxSizePercent)\n",
    "\n",
    "  # as background's width before applying this random scale percent (should really be same as shortest side?)\n",
    "  # baseScale = backgroundWidth / objectWidth # The baseScale is the ratio of how big background's width is compared to object's width (ONLY WITH MEASUREMENTS)\n",
    "  if backgroundWidth <= backgroundHeight: # scale based on shortest side of background\n",
    "    baseScale = backgroundWidth/objectWidth\n",
    "    used = \"width shortest so base scale is based on width\"\n",
    "  else:\n",
    "    baseScale = backgroundHeight/objectHeight\n",
    "    used = \"height shortest so base scale is based on height\"\n",
    "\n",
    "  # print({\n",
    "  #   \"backgroundWidth\": backgroundWidth,\n",
    "  #   \"object.width\": object.width,\n",
    "  #   \"objectWidth\": objectWidth,\n",
    "  #   \"baseScale\": baseScale,\n",
    "  #   \"scalePercent\": scalePercent,\n",
    "  #   \"baseScale * scalePercent\": baseScale * scalePercent,\n",
    "  #   \"object.width * scalePercent/100\": object.width * scalePercent/100,\n",
    "  #   \"object.height * scalePercent/100\": object.height * scalePercent/100,\n",
    "  # })\n",
    "\n",
    "\n",
    "  scalePercent = baseScale * scalePercent # fix the scalePercent to include the baseScale between the 2 images and be proportional\n",
    "\n",
    "  # Choose a random position for object in background\n",
    "  widthOfObjectAfterScaling = int(object.width * scalePercent)   # predict width of object after scaling so you can choose a random width in bounds of background\n",
    "  heightOfObjectAfterScaling = int(object.height * scalePercent) # predict height of object after scaling so you can choose a random height in bounds of background\n",
    "  x = random.randint(0, backgroundWidth - widthOfObjectAfterScaling)     # select random x position for object in background\n",
    "\n",
    "  # ERROR IS HERE: The error is because background isn't tall enough to fit object even after scaling so the randint is trying to selced from 0 to a negative number\n",
    "  y = random.randint(0, backgroundHeight - heightOfObjectAfterScaling)   # select random y position for object in background\n",
    "\n",
    "  (combinedImage, finalHeight) = stackAndScaleImage(objectPath, backgroundPath, scalePercent, (x, y)) # Use the stack_scaled_images function to combine the two images\n",
    "\n",
    "  # finalWidth, finalHeight = combinedImage.size # WRONG\n",
    "\n",
    "  # Save the paste_parameters as a json object\n",
    "  debugParameters = {\n",
    "    \"width_background\": backgroundWidth,\n",
    "    \"height_background\": backgroundHeight,\n",
    "    \"paste_x\": x,\n",
    "    \"paste_y\": y,\n",
    "    \"paste_width\": widthOfObjectAfterScaling,\n",
    "    \"paste_height\": finalHeight,\n",
    "    \"scale_type\": used\n",
    "  }\n",
    "  # The position for object to be pasted onto background is randomly selected within the bounds of background\n",
    "\n",
    "  # https://docs.cogniflow.ai/en/article/how-to-create-a-dataset-for-object-detection-using-the-yolo-labeling-format-1tahk19/\n",
    "  # YOLO labeling parameters\n",
    "  pasteParametersYolo = {\n",
    "    \"objectClassNum\": CLASSES[objectClassStr],\n",
    "    \"x_center\": (x + widthOfObjectAfterScaling/2.0) / backgroundWidth, # calculate what percentage of the width of background the center of scaled object will be\n",
    "    \"y_center\": (y + finalHeight/2.0) / backgroundHeight, # calculate what percentage of the height of background the center of scaled object will be\n",
    "    \"width\": widthOfObjectAfterScaling / backgroundWidth, # calculate what percentage of the width of background does scaled object take\n",
    "    \"height\": finalHeight / backgroundHeight, # calculate what percentage of the height of background does scaled object take\n",
    "  }\n",
    "\n",
    "  return (combinedImage, debugParameters, pasteParametersYolo) # return the PIL image object after stacking, the parameters used for pasting, and the parameters used for pasting in a YOLO suitable notation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def combineRandomImages(directory1, directory2, minSizePercent, maxSizePercent, CLASSES):\n",
    "  # Combines two random images from the two directories using the first function, returns a PIL Image object with the combined images\n",
    "\n",
    "  #   directory1: Path to the first directory.\n",
    "  #   directory2: Path to the second directory.\n",
    "  #   minSizePercent: Minimum percentage to scale down the first image.\n",
    "  #   maxSizePercent: Maximum percentage to scale down the first image.\n",
    "\n",
    "  # Get all files in directories. If one isn't an image it could break\n",
    "  objectFiles = os.listdir(directory1) # Get a list of all files in the first directory\n",
    "  backgroundFiles = os.listdir(directory2) # Get a list of all files in the second directory\n",
    "\n",
    "  imageChosen = random.choice(objectFiles)\n",
    "  objectClass = imageChosen.split(\"_\")[0]\n",
    "\n",
    "  # Choose random images\n",
    "  objectPath = os.path.join(directory1, imageChosen) # Choose a random file from the first directory\n",
    "  backgroundPath = os.path.join(directory2, random.choice(backgroundFiles)) # Choose a random file from the second directory\n",
    "\n",
    "  # Use the combine_images_james_xy function to combine the two images.\n",
    "  combinedImage, debugParameters, pasteParametersYolo = selectScaleAndCreateYoloLabels(objectPath, backgroundPath, minSizePercent, maxSizePercent, objectClass, CLASSES)\n",
    "\n",
    "  # print(pasteParametersYolo)\n",
    "\n",
    "  return (combinedImage, debugParameters, pasteParametersYolo) # re-return all of the returns from the combine_images_james_xy function\n",
    "\n",
    "# def makeImage(testOrTrain=\"test\", numImages=10, minSizePercent=.05, maxSizePercent=.8, i=-1):\n",
    "def makeImage(args):\n",
    "  (testOrTrain, numImages, minSizePercent, maxSizePercent, CLASSES, i) = args\n",
    "  if i == -1:\n",
    "    raise ValueError(\"Invalid i Value\")\n",
    "  # Combine the images from directory1 (game object) with images from directory2 (backgrounds).\n",
    "  combinedImage, debugParameters, pasteParametersYolo = combineRandomImages(\"./objectImages\", \"./VOCdevkit/\"+testOrTrain, minSizePercent, maxSizePercent, CLASSES)\n",
    "  # Figure out a file name based on the current iteration and type of dataset\n",
    "  baseFilename = f\"{testOrTrain}_{i:0{6}d}\" # Max 100000 file names\n",
    "  # Save the image to the specified folder based on type of data set and use the above created filename\n",
    "  combinedImage.save('./ultralytics/Dataset/images/'+testOrTrain+'/'+baseFilename+'.png')\n",
    "\n",
    "  # Open/create a text file with the same name as the image and add the paste_parameters_yolo to it\n",
    "  with open('./ultralytics/Dataset/labels/'+testOrTrain+'/'+baseFilename+'.txt', \"w\") as f:\n",
    "    yoloData = pasteParametersYolo\n",
    "    if round(yoloData['x_center'],6) > 1 or round(yoloData['y_center'],6) > 1:\n",
    "      print(\"ERROR at\", i)\n",
    "      print(yoloData)\n",
    "      print(debugParameters)\n",
    "    f.write(f\"{yoloData['objectClassNum']} {round(yoloData['x_center'],6)} {round(yoloData['y_center'],6)} {round(yoloData['width'],6)} {round(yoloData['height'],6)}\") # write data and round it to the correct decimal places (first digit in string is the class)\n",
    "\n",
    "\n",
    "\n",
    "# Main function that ties together all of the other functions to make it work\n",
    "def makeData(testOrTrain=\"test\", numImages=10, minSizePercent=.05, maxSizePercent=.8):\n",
    "  if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--classes\", type=str)\n",
    "    parser.add_argument(\"--testOrTrain\", default=\"test\", type=str)\n",
    "    parser.add_argument(\"--numImages\", default=2, type=int)\n",
    "    parser.add_argument(\"--minSizePercent\", default=.05, type=float)\n",
    "    parser.add_argument(\"--maxSizePercent\", default=.8, type=float)\n",
    "\n",
    "  inputArgs = parser.parse_args()\n",
    "\n",
    "  CLASSES = json.loads(inputArgs.classes.replace(\"'\", '\"'))\n",
    "  testOrTrain = inputArgs.testOrTrain\n",
    "  numImages = inputArgs.numImages\n",
    "  minSizePercent = inputArgs.minSizePercent\n",
    "  maxSizePercent = inputArgs.maxSizePercent\n",
    "\n",
    "  testOrTrain = testOrTrain.lower()\n",
    "  \n",
    "\n",
    "  args = tuple([(testOrTrain, numImages, minSizePercent, maxSizePercent, CLASSES, i) for i in range(numImages)]) # creates a tuple of tuple\n",
    "  \n",
    "  with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    executor.map(makeImage, args)\n",
    "    print(\"yippie!!! it works\") # to make sure it works :)\n",
    "  ##########\n",
    "  # for i in os.listdir(\"./ultralytics/Dataset/labels/train\"):\n",
    "  #   with open(\"./ultralytics/Dataset/labels/train/\"+i, 'rt') as f:\n",
    "  #     vals = f.read()\n",
    "  #     vals = vals.split(\" \")\n",
    "  #     vals = list(map(float, vals))\n",
    "  #     if vals[1] > 1 or vals[2] > 1:\n",
    "  #       print(i, vals[1], vals[2], vals[3], vals[4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the test and train image sets. You should pick a size percent that makes sense for the application. I suggest doing some tests by just generating 50 images and then seeing how it looks. Here are the lines to make a test and train dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ./dataGenTrain.py\n",
    "# from dataGen import *\n",
    "!python3 dataGen.py --classes=\"$CLASSES\" --testOrTrain=\"train\" --numImages=15000 --minSizePercent=.1 --maxSizePercent=.8\n",
    "# args = tuple([(\"train\", 15000, 0.10, 0.80, \"$CLASSES\", i) for i in range(15000)]) # creates a tuple of tuple\n",
    "# with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "#     executor.map(makeImage, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 dataGenTrain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ./dataGenTest.py\n",
    "# from dataGen import *\n",
    "!python3 dataGen.py --classes=\"$CLASSES\" --testOrTrain=\"test\" --numImages=7500 --minSizePercent=.1 --maxSizePercent=.9\n",
    "# args = tuple([(\"test\", 7500, 0.10, 0.90, \"$CLASSES\", i) for i in range(7500)]) # creates a tuple of tuple\n",
    "# with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "#     executor.map(makeImage, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 dataGenTest.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the dataset config file for training yolo but make sure to enumerate the classes at the bottom and pick names that are appropriate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./ultralytics/cfg/datasets/CUSTOM.yaml\n",
    "# Ultralytics YOLO ðŸš€, AGPL-3.0 license\n",
    "# Documentation: # Documentation: https://docs.ultralytics.com/datasets/detect/voc/\n",
    "# Example usage: yolo train data=CUSTOM.yaml\n",
    "# parent\n",
    "# â”œâ”€â”€ ultralytics\n",
    "# â””â”€â”€ datasets\n",
    "#     â””â”€â”€ CUSTOM\n",
    "\n",
    "# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\n",
    "path: ./Dataset\n",
    "train: images/train # train images (relative to 'path')  16551 images\n",
    "val: images/test # val images (relative to 'path')  4952 images\n",
    "test: images/test # test images (optional)\n",
    "\n",
    "# Set the number of classes\n",
    "nc: 1 # EDIT TO REFLECT CORRECT VALUES\n",
    "\n",
    "# Classes\n",
    "names: # EDIT HERE\n",
    "  0: algae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model\n",
    "\n",
    "Now that we have the dataset we can train the model. First we will run the checks command to make sure everything is good (it has all the libraries, the computer is able to train it, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm we have pyTorch available on our system (prints as a boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is good we will run the help command to see how to use the yolo command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the nano version of yolov11 because it is the smallest, fastest, and most lightweight version. We will specify the name of the custom data config file that we created above as well as yolov11n.pt for the nano version of the pretrained model. I've decided on 10 epochs for just a short retraining of the model. The imgsz is 640 which will resize all of the images we created to 640x640 for input into the model. This number can be changed for speed over accuracy if needed. I set the batch size to use 90% of the available GPU memory as well as set cache to true to improve speed.\n",
    "\n",
    "my face when yolov12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo settings datasets_dir=ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imgsz is set to 512 because it must be a multiple of 32 and not greater than 530 to work with the camera with the OV... sensor\n",
    "\n",
    "julia note to be updated: if youre not running on gpu machine, set device=cpu. otherwise keep it at device=0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---------\n",
    "\n",
    "\n",
    "if training for the first time this is gonna take a while (as in I started running it at 12 am and it is now 5 am)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo detect train data=\"./ultralytics/cfg/datasets/CUSTOM.yaml\" model=\"yolo11n.pt\" epochs=2 imgsz=512 name=\"lakeMONSTER;)\" batch=0.5 patience=2 cache=disk workers=8 device=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Upload to Luxonis to convert to OpenVINO format\n",
    "\n",
    "### NOTE: tools.luxonis.com is no longer maintained and is not compatible with versions past Yolov11 or depthAIv3. (How unfortunate that those are both versions we want to upgrade our code to...)\n",
    "\n",
    "Navigate to [tools.luxonis.com](https://tools.luxonis.com) and select the following parameters:\n",
    "\n",
    "- Yolo Version: `YOLOv11`\n",
    "- `RVC2`\n",
    "- Upload your best.pt file (when you finish running the above code it prints where the files are... runs/detect/`name`/weights)\n",
    "  - We use best.pt because it is literally the best out of the rest... last.pt is just the last trained epoch, since there may be situations where too much training actually makes it worse, so the best isn't one of the last few epochs. Most of the time they're pretty similar but we should use best.pt\n",
    "- Image shape: `512`\n",
    "- Shaves: `6`\n",
    "- Use OpenVINO: `True`\n",
    "\n",
    "When it finishes processing, you will get a `result.zip` file. Open it and replace everything in the `models` folder with the content of the `result` folder.\n",
    "\n",
    "## 9. Export to Pi\n",
    "something with https://wpilibpi.local:1181 idk tho\n",
    "\n",
    "instructions below copied from `~/MonsterVision5/README.md`\n",
    "\n",
    "### How to toggle the camera server\n",
    "1. Open command prompt\n",
    "2. `ssh pi@wpilibpi` or `ssh pi@wpilibpi.local`\n",
    "3. Go to [wpilibpi.local webserver](http://wpilibpi.local/) and change it to writable\n",
    "4. `sudo nano /boot/mv.json`\n",
    "5. Change the `'showPreview'` key to `false` or `true` depending on whether or not you need it\n",
    "6. Restart MonsterVision\n",
    "\n",
    "How to Restart MonsterVision:\n",
    "1. Go to [wpilibpi.local webserver](http://wpilibpi.local/) and go to Vision Status\n",
    "2. Click the red Kill button\n",
    "\n",
    "IF KILL/TERMINATE doesn't seem to work then you have to go into htop and SIGKILL the main MonsterVision4.5.py:\n",
    "1. Open command prompt\n",
    "2. `ssh pi@wpilibpi` or `ssh pi@wpilibpi.local`\n",
    "3. Go to [the wpilibpi.local webserver](http://wpilibpi.local/) and change it to writable\n",
    "4. `htop` in terminal to view all processes\n",
    "5. Click on the `python3 ./MonsterVision4.5.py` (should be in green)\n",
    "6. Type `fn+f9` and then '9' to execute the SIGKILL command to kill the process\n",
    "7. Hit enter to execute the command\n",
    "\n",
    "\n",
    "### How to change model used\n",
    "1. Open command prompt\n",
    "2. `ssh pi@wpilibpi` or `ssh pi@wpilibpi.local`\n",
    "3. Go to [wpilibpi.local webserver](http://wpilibpi.local/) and change it to writable\n",
    "4. Type `cd ./<path to MonsterVision>/models`\n",
    "6. Type `sudo cp ./<desired model .json file> /boot/nn.json`\n",
    "7. Type `mv ./<latest best.blob> ./<appropriate name for .blob given season>`\n",
    "8. Type `sudo nano /boot/nn.json`\n",
    "9. Add between lines 6 and 7 (6.5) `\"blob\": \"<chosen appropriate name given season>\", `\n",
    "\n",
    "6 7?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
