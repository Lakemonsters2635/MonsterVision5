{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Data Generation for YOLOv7+ with VOCDataset and Training of YOLOv12\n",
    "\n",
    "1. Take pictures\n",
    "1. Prepare environment\n",
    "1. Prepare/Organize object images\n",
    "3. Download VOCDataset for backing of object images\n",
    "1. Overlay objects on backing images and generate YOLO labels\n",
    "9000. Train model\n",
    "1. Upload to Luxonis to convert to OpenVINO format\n",
    "\n",
    "Note: the `os.chdir()` function at the beginning of most code cells is necessary for users to know what directory they're in, and to make sure functions are being executed in the correct directory.\n",
    "\n",
    "### TO DO:\n",
    "- find a new fix for the last step\n",
    "- test new os functions on linux and windows\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Take Pictures\n",
    "\n",
    "When taking pictures of objects:\n",
    "- TAKE MANY PHOTOS AT ALL POSSIBLE ANGLES (15-ish photos, depending on how many angles we need)\n",
    "- Good lighting and quality, of course\n",
    "- Try to have the object be the only thing in frame (no extra objects that could possibly take away focus from the image)\n",
    "- The object should be completely in frame and not cut off\n",
    "- When you get all these images, place them in `~/MonsterVision5/uneditedObjects/`\n",
    "\n",
    "Examples of some image angles/variation:\n",
    "\n",
    "<img src=\"markdownimages/uneditedimage.png\" alt=\"goodexample-baseimage\" width=\"900\" height=\"300\">\n",
    "\n",
    "For testing purposes, there are some sample images in the `sample-images/` folder with algaes, corals, notes, and carrots, you can duplicate the contents (but do not remove them!)\n",
    "\n",
    "--------\n",
    "\n",
    "## 2. Prepare Environment\n",
    "\n",
    "<img src=\"markdownimages/image.png\" alt=\"goodexample-baseimage\" width=\"550\" height=\"200\">\n",
    "\n",
    "### IMPORTANT THINGS TO NOTE WHEN RUNNING THIS BLOCK:\n",
    "1. this cell block must be run first before anything else because it sets our default directory and creates the kernel!!!\n",
    "1. If you don't have a kernel already selected, select python 3.10 before running (YOU CANNOT USE ANOTHER VERSION)!\n",
    "1. after running the code block, you will now have a venv! change the kernel again to python VENV 3.10\n",
    "1. run this block one more time after changing to the venv (when we switch kernels it restarts/loses our variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 --version\n",
    "!python3.10 -m venv .venv\n",
    "!source .venv/bin/activate\n",
    "\n",
    "import os\n",
    "absoluteMVpath = os.getcwd()\n",
    "os.chdir(absoluteMVpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we will make directories and get ultralytics :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(absoluteMVpath)\n",
    "# you need to import os again if you change the kernel :)\n",
    "!git clone --progress --verbose https://github.com/ultralytics/ultralytics\n",
    "\n",
    "# we do not cd into ultralytics yet since we complete other object image tasks first\n",
    "# operations are inside ultralytics\n",
    "# create other folders that we will need later:\n",
    "\n",
    "# -p checks if the directory already exists before making it\n",
    "!mkdir -p ./ultralytics/objectImages\n",
    "!mkdir -p ./ultralytics/ultralytics/Dataset\n",
    "!mkdir -p ./ultralytics/ultralytics/Dataset/images\n",
    "!mkdir -p ./ultralytics/ultralytics/Dataset/labels\n",
    "!mkdir -p ./ultralytics/ultralytics/Dataset/images/test\n",
    "!mkdir -p ./ultralytics/ultralytics/Dataset/images/train\n",
    "!mkdir -p ./ultralytics/ultralytics/Dataset/labels/test\n",
    "!mkdir -p ./ultralytics/ultralytics/Dataset/labels/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CUDA version if you have CUDA for pytorch installation and install dependencies for yolov11 and all other ultralytics yolo versions:\n",
    "\n",
    "For pytorch: \n",
    "**Visit [pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/) to install the correct version of pytorch** (only change the url part of the command and if you have a newer version of CUDA than pytorch has then install the latest pytorch version). This could take a while depending on your network speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(absoluteMVpath)\n",
    "import subprocess\n",
    "isCuda = False\n",
    "isWindows = False\n",
    "\n",
    "# Define the command as a list of strings\n",
    "# Example: 'ls -l' would be ['ls', '-l']\n",
    "# capture_output should be true if you want the output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CHANGE URL BELOW:\n",
    "try:\n",
    "    subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "    # if machine is cuda\n",
    "    isCuda = True\n",
    "    print(\"cuda machine\")\n",
    "    %pip install \"ultralytics\" \"tqdm>=4.41.0\" \"pillow\" \"pip install rembg[gpu,cli]==2.0.28\"\n",
    "    %pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n",
    "except:\n",
    "    # if machine is non-cuda\n",
    "    isCuda = False\n",
    "    print(\"non-cuda machine\")\n",
    "    %pip install \"ultralytics\" \"tqdm>=4.41.0\" \"pillow\" \"rembg==2.0.28\"\n",
    "    %pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    subprocess.run([\"ver\"], capture_output=True, text=True)\n",
    "    # if machine is windows\n",
    "    isWindows = True\n",
    "    print(\"windows user!\")\n",
    "except:\n",
    "    # if machine is NOT!!!! windows\n",
    "    isWindows = False\n",
    "    print (\"mac/linux user\")\n",
    "print(\"cuda machine:\", isCuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Object Images\n",
    "\n",
    "Next we will clean up the data (images) we have by organizing/editing them\n",
    "\n",
    "Example: (note how there is no background and it is cropped right to the edge of the object)\n",
    "\n",
    "ORIGINAL:\n",
    "<img src=\"markdownimages/algae_6411.jpeg\" alt=\"unedited\" width=\"300\" height=\"390\">\n",
    "EDITED:\n",
    "<img src=\"markdownimages/editedimage.png\" alt=\"goodexample-editedimage\" width=\"300\" height=\"300\">\n",
    "\n",
    "---------\n",
    "\n",
    "#### BEFORE RUNNING THIS CODE CELL: Organize your images in folders within the original folder \n",
    "- Within `uneditedObjects`, make folders with the names of each gamepiece you are using and place each image in its respective directory.\n",
    "- **EVEN IF YOU HAVE ONLY ONE TYPE OF GAMEPIECE**, YOU MUST STILL PUT IT IN A FOLDER WITH ITS NAME!\n",
    "- example: if your object images are `algae` and `coral`, your directory should look something like this:\n",
    "```\n",
    "MonsterVision5\n",
    "â”œâ”€â”€ uneditedObjects\n",
    "|     â”œâ”€â”€ algae\n",
    "|     |     â””â”€â”€ images of algaes go here\n",
    "|     â””â”€â”€ coral\n",
    "|           â””â”€â”€ images of corals go here\n",
    "|\n",
    "â””â”€â”€â”€â”€â”€ other folders we dont care about right now\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "os.chdir(absoluteMVpath)\n",
    "\n",
    "classNames = os.listdir('./uneditedObjects/')\n",
    "CLASSES = {key: index for index, key in enumerate(classNames)} # DON'T EDIT\n",
    "os.environ['CLASSES'] = json.dumps(CLASSES)\n",
    "print(json.dumps(CLASSES))\n",
    "\n",
    "print(\"renaming images...\")\n",
    "# rename all images to fit the naming conventions\n",
    "for object in os.listdir('./uneditedObjects/'):\n",
    "    number = 0\n",
    "    for image in os.listdir('./uneditedObjects/' + object):\n",
    "        os.rename('./uneditedObjects/' + object + '/' + image, './uneditedObjects/' + object + '_' + str(number) + '.png')\n",
    "        number += 1\n",
    "\n",
    "# we first use rembg to remove background of images from our base images\n",
    "# THEN we put them through Pillow to crop the images using the detected removed background\n",
    "# then it saves it in ultralytics\n",
    "\n",
    "print(\"removing background...\")\n",
    "!mkdir ./temporary/\n",
    "!rembg p ./uneditedObjects temporary\n",
    "\n",
    "print(\"cropping images...\")\n",
    "for i in os.listdir('./temporary/'):\n",
    "    img = Image.open(f\"./temporary/{i}\").convert(\"RGBA\") # MUST BE PNG FILE TO USE RGBA >:/\n",
    "    alpha = img.split()[3] # alpha channel is apparently the 4th channel in RGBA, it means transparency\n",
    "    # threshold makes sure that even if there are translucent pixels in the background we crop them\n",
    "    threshold = 10  # tweak this value if needed (0â€“255), 10 is pretty good for now\n",
    "    alpha = alpha.point(lambda p: p > threshold and 255)\n",
    "    # alpha.point is a function applied to all pixels in an image\n",
    "    # the threshold checks how translucent it is and either makes it fully transparent or opaque\n",
    "    img.putalpha(alpha) # recombine to an RGBA image\n",
    "    bbox = alpha.getbbox() # find the bounding box of non-transparent pixels (find the edge of where there actually is an image object)\n",
    "\n",
    "    if bbox:\n",
    "        cropped = img.crop(bbox)\n",
    "        cropped.save(f\"./ultralytics/objectImages/{i}\") # the Final Destination (haha) of the images\n",
    "        \n",
    "print(\"images are now in /ultralytics/objectImages/!\\nplease confirm that EVERY image has been edited correctly\\n\\n\\n\\n\\n\\n\")\n",
    "if not isWindows:\n",
    "    isWindows = False\n",
    "    !rm -rf ./temporary/\n",
    "    !rm -rf ./uneditedObjects/*\n",
    "else:\n",
    "    isWindows = True\n",
    "    !rmdir ./temporary/\n",
    "    !rmdir ./uneditedObjects/\n",
    "    !mkdir ./uneditedObjects/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**before proceeding, please check the images to make sure they have been edited correctly (they are in `/ultralytics/objectImages/`)**\n",
    "## 4. Download VOCDataset for Backing of Object Images\n",
    "We use the VOCDataset or Visualized Object Classes Dataset which is a dataset that contains many images with labels for training of pascal. We are just extracting the images from a few of the datasets for backing images for our yolo training images.\n",
    "\n",
    "Download the 2007 and 2012 VOCDataset and put them in a separate directory (may take 10+ minutes depending on wifi) and extract the downloaded .tar archive files (should create folders `VOC2007` and `VOC2012` in `VOCdevkit`. If not then run again):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(absoluteMVpath + \"/ultralytics\")\n",
    "!mkdir -p ./VOCdevkit \n",
    "!mkdir -p ./VOCdevkit/test\n",
    "!mkdir -p ./VOCdevkit/train\n",
    "!curl -L -o ./VOCdevkit/pascal-voc-2012-dataset https://www.kaggle.com/api/v1/datasets/download/gopalbhattrai/pascal-voc-2012-dataset\n",
    "\n",
    "if isWindows:\n",
    "    isWindows = True\n",
    "    !tar -xvf ./VOCdevkit/pascal-voc-2012-dataset.zip -C ./VOCdevkit/\n",
    "else:\n",
    "    isWindows = False\n",
    "    !unzip ./VOCdevkit/pascal-voc-2012-dataset -d ./VOCdevkit/\n",
    "\n",
    "\n",
    "os.chdir(absoluteMVpath + \"/ultralytics/VOCdevkit\")\n",
    "for i in os.listdir('./VOC2012_test/VOC2012_test/JPEGImages'): # listdir wil return a list of strings of every file and folder name in the DIR\n",
    "    os.rename('./VOC2012_test/VOC2012_test/JPEGImages/' + i, './test/' + i) # takes original path and appends each image file then moves it by \"renaming\" it\n",
    "for i in os.listdir('./VOC2012_train_val/VOC2012_train_val/JPEGImages'):\n",
    "    os.rename('./VOC2012_train_val/VOC2012_train_val/JPEGImages/' + i, './train/' + i)\n",
    "os.chdir(absoluteMVpath + \"/ultralytics/VOCdevkit\")\n",
    "\n",
    "\n",
    "if isWindows:\n",
    "    isWindows = True\n",
    "    !rmdir ./VOC2012_test/\n",
    "    !rmdir ./VOC2012_train_val/\n",
    "    !del pascal-voc-2012-dataset\n",
    "else:\n",
    "    isWindows = False\n",
    "    !rm -rf ./VOC2012_test/\n",
    "    !rm -rf ./VOC2012_train_val/\n",
    "    !rm pascal-voc-2012-dataset\n",
    "\n",
    "os.chdir(absoluteMVpath + \"/ultralytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Overlay objects on backing images and generate YOLO labels\n",
    "\n",
    "This script automatically generates synthetic training data for object detection models (such as YOLO) by compositing foreground object images onto random background images.\n",
    "\n",
    "### Overview:\n",
    "1. Randomly selects an object image (with transparent background) and a random background image.\n",
    "2. Scales the object to a random percentage of the backgroundâ€™s size.\n",
    "3. Pastes the scaled object onto a random position within the background.\n",
    "4. Saves the resulting composite image to the dataset folder (e.g., ./ultralytics/Dataset/images/...).\n",
    "5. Generates a corresponding YOLO-format label (.txt) containing the objectâ€™s class ID and bounding box coordinates\n",
    "   (normalized x_center, y_center, width, height).\n",
    "6. Designed for multiprocessing to efficiently generate large datasets.\n",
    "\n",
    "### Functions:\n",
    "- stackAndScaleImage(): Scales and pastes one image onto another using PIL.\n",
    "- selectScaleAndCreateYoloLabels(): Combines object and background, computes YOLO label data.\n",
    "- combineRandomImages(): Picks random images from directories and combines them.\n",
    "- makeImage(): Creates and saves a single labeled synthetic training image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(absoluteMVpath + \"/ultralytics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./dataGen.py \n",
    "# we write to a new file so we can do multiprocessing on a separate python file (we cant do multiprocessing in .ipynb)\n",
    "from PIL import Image\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm as progressBar\n",
    "import threading\n",
    "import multiprocessing\n",
    "import concurrent.futures\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "def stackAndScaleImage(objectImage, backgroundImage, scalePercent, position): # Takes a first PNG image, scales it down a certain percentage and pastes it on a second PNG image.\n",
    "\n",
    "# objectImage: PIL image of object\n",
    "# backgroundImage: PIL image of background\n",
    "# scalePercent: Percentage to scale down the first image. (MAKE FLOAT INSTEAD OF DUMB STUFF)\n",
    "# position: Tuple of (x, y) coordinates to paste the scaled image.\n",
    "\n",
    "  objectImage = Image.open(objectImage)\n",
    "  backgroundImage = Image.open(backgroundImage)\n",
    "\n",
    "  # Scale down the first image\n",
    "  width, height = objectImage.size # extract the width and height of object\n",
    "  newWidth = int(width * scalePercent) # create a new width for object based on the scalePercent and makes it an int\n",
    "  newHeight = int(height * scalePercent) # create a new height for object based on the scalePercent and makes it an int\n",
    "  objectScaled = objectImage.resize((newWidth, newHeight)) # use the resize method of a PIL Image to scale object to the new_width and new_height\n",
    " \n",
    "  # Scaled the hight to add more variation to the data\n",
    "  width, height = objectScaled.size # Reset the width and height variables to be the new width and height of object after scaling\n",
    "  randomHeightScale = random.uniform(0.8, 1.2) # Choose a random scalePercent between the minimum and maximum values.\n",
    "  # randomHeightScale = 1.0\n",
    "  newHeight = int(height * randomHeightScale) # Calculate the newHeight based on the random scalePercent above\n",
    "  if newHeight > backgroundImage.height:\n",
    "    newHeight = backgroundImage.height\n",
    "    print(\"triggered\", newHeight)\n",
    "  objectScaled = objectScaled.resize((width, newHeight)) # Resize the image just like above\n",
    "  \n",
    "  backgroundImage.paste(objectScaled, position, objectScaled) # Second parameter makes it so that the pixels with no value meant to be clear stay clear and aren't black\n",
    "\n",
    "  return (backgroundImage, newHeight) # Return the final stacked image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SCALE PERCENTAGE IS HOW MUCH OF THE FRAME YOU WANT TO TAKE UP NOT HOW MUCH YOU WANT TO SCALE THE OBJECT IMAGE DOWN\n",
    "def selectScaleAndCreateYoloLabels(objectPath, backgroundPath, minSizePercent, maxSizePercent, objectClassStr, CLASSES): \n",
    "  # Combines two images by pasting a scaled version of an object onto a background. SCALES BASED ON WIDTH\n",
    "\n",
    "  # objectPath: Path to the object image.\n",
    "  # backgroundPath: Path to the background image.\n",
    "  # minSizePercent: Minimum percentage to scale down object's WIDTH.\n",
    "  # maxSizePercent: Maximum percentage to scale down object's WIDTH.\n",
    "\n",
    "  object = Image.open(objectPath) # Open the first image as object\n",
    "  background = Image.open(backgroundPath) # Open the second image as background\n",
    "\n",
    "  objectWidth, objectHeight = object.size # Extract the width and height of object as objectWidth and objectHeight\n",
    "  backgroundWidth, backgroundHeight = background.size # Extract the width and height of background as backgroundWidth and backgroundHeight\n",
    "\n",
    "  # Choose a random scalePercent between the minimum and maximum values provided as parameters\n",
    "  # NEVER ABOVE 1.0(?)\n",
    "  scalePercent = random.uniform(minSizePercent, maxSizePercent)\n",
    "\n",
    "  # as background's width before applying this random scale percent (should really be same as shortest side?)\n",
    "  # baseScale = backgroundWidth / objectWidth # The baseScale is the ratio of how big background's width is compared to object's width (ONLY WITH MEASUREMENTS)\n",
    "  if backgroundWidth <= backgroundHeight: # scale based on shortest side of background\n",
    "    baseScale = backgroundWidth/objectWidth\n",
    "    used = \"width shortest so base scale is based on width\"\n",
    "  else:\n",
    "    baseScale = backgroundHeight/objectHeight\n",
    "    used = \"height shortest so base scale is based on height\"\n",
    "\n",
    "  # print({\n",
    "  #   \"backgroundWidth\": backgroundWidth,\n",
    "  #   \"object.width\": object.width,\n",
    "  #   \"objectWidth\": objectWidth,\n",
    "  #   \"baseScale\": baseScale,\n",
    "  #   \"scalePercent\": scalePercent,\n",
    "  #   \"baseScale * scalePercent\": baseScale * scalePercent,\n",
    "  #   \"object.width * scalePercent/100\": object.width * scalePercent/100,\n",
    "  #   \"object.height * scalePercent/100\": object.height * scalePercent/100,\n",
    "  # })\n",
    "\n",
    "\n",
    "  scalePercent = baseScale * scalePercent # fix the scalePercent to include the baseScale between the 2 images and be proportional\n",
    "\n",
    "  # Choose a random position for object in background\n",
    "  widthOfObjectAfterScaling = int(object.width * scalePercent)   # predict width of object after scaling so you can choose a random width in bounds of background\n",
    "  heightOfObjectAfterScaling = int(object.height * scalePercent) # predict height of object after scaling so you can choose a random height in bounds of background\n",
    "  x = random.randint(0, backgroundWidth - widthOfObjectAfterScaling)     # select random x position for object in background\n",
    "\n",
    "  # ERROR IS HERE: The error is because background isn't tall enough to fit object even after scaling so the randint is trying to selced from 0 to a negative number\n",
    "  y = random.randint(0, backgroundHeight - heightOfObjectAfterScaling)   # select random y position for object in background\n",
    "\n",
    "  (combinedImage, finalHeight) = stackAndScaleImage(objectPath, backgroundPath, scalePercent, (x, y)) # Use the stack_scaled_images function to combine the two images\n",
    "\n",
    "  # finalWidth, finalHeight = combinedImage.size # WRONG\n",
    "\n",
    "  # Save the paste_parameters as a json object\n",
    "  debugParameters = {\n",
    "    \"width_background\": backgroundWidth,\n",
    "    \"height_background\": backgroundHeight,\n",
    "    \"paste_x\": x,\n",
    "    \"paste_y\": y,\n",
    "    \"paste_width\": widthOfObjectAfterScaling,\n",
    "    \"paste_height\": finalHeight,\n",
    "    \"scale_type\": used\n",
    "  }\n",
    "  # The position for object to be pasted onto background is randomly selected within the bounds of background\n",
    "\n",
    "  # https://docs.cogniflow.ai/en/article/how-to-create-a-dataset-for-object-detection-using-the-yolo-labeling-format-1tahk19/\n",
    "  # YOLO labeling parameters\n",
    "  pasteParametersYolo = {\n",
    "    \"objectClassNum\": CLASSES[objectClassStr],\n",
    "    \"x_center\": (x + widthOfObjectAfterScaling/2.0) / backgroundWidth, # calculate what percentage of the width of background the center of scaled object will be\n",
    "    \"y_center\": (y + finalHeight/2.0) / backgroundHeight, # calculate what percentage of the height of background the center of scaled object will be\n",
    "    \"width\": widthOfObjectAfterScaling / backgroundWidth, # calculate what percentage of the width of background does scaled object take\n",
    "    \"height\": finalHeight / backgroundHeight, # calculate what percentage of the height of background does scaled object take\n",
    "  }\n",
    "\n",
    "  return (combinedImage, debugParameters, pasteParametersYolo) # return the PIL image object after stacking, the parameters used for pasting, and the parameters used for pasting in a YOLO suitable notation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def combineRandomImages(directory1, directory2, minSizePercent, maxSizePercent, CLASSES):\n",
    "  # Combines two random images from the two directories using the first function, returns a PIL Image object with the combined images\n",
    "\n",
    "  #   directory1: Path to the first directory.\n",
    "  #   directory2: Path to the second directory.\n",
    "  #   minSizePercent: Minimum percentage to scale down the first image.\n",
    "  #   maxSizePercent: Maximum percentage to scale down the first image.\n",
    "\n",
    "  # Get all files in directories. If one isn't an image it could break\n",
    "  objectFiles = os.listdir(directory1) # Get a list of all files in the first directory\n",
    "  backgroundFiles = os.listdir(directory2) # Get a list of all files in the second directory\n",
    "\n",
    "  imageChosen = random.choice(objectFiles)\n",
    "  objectClass = imageChosen.split(\"_\")[0]\n",
    "\n",
    "  # Choose random images\n",
    "  objectPath = os.path.join(directory1, imageChosen) # Choose a random file from the first directory\n",
    "  backgroundPath = os.path.join(directory2, random.choice(backgroundFiles)) # Choose a random file from the second directory\n",
    "\n",
    "  # Use the combine_images_james_xy function to combine the two images.\n",
    "  combinedImage, debugParameters, pasteParametersYolo = selectScaleAndCreateYoloLabels(objectPath, backgroundPath, minSizePercent, maxSizePercent, objectClass, CLASSES)\n",
    "\n",
    "  # print(pasteParametersYolo)\n",
    "\n",
    "  return (combinedImage, debugParameters, pasteParametersYolo) # re-return all of the returns from the combine_images_james_xy function\n",
    "\n",
    "# def makeImage(testOrTrain=\"test\", numImages=10, minSizePercent=.05, maxSizePercent=.8, i=-1):\n",
    "def makeImage(args):\n",
    "  (testOrTrain, numImages, minSizePercent, maxSizePercent, CLASSES, i) = args\n",
    "  if i == -1:\n",
    "    raise ValueError(\"Invalid i Value\")\n",
    "  # Combine the images from directory1 (game object) with images from directory2 (backgrounds).\n",
    "  combinedImage, debugParameters, pasteParametersYolo = combineRandomImages(\"./objectImages\", \"./VOCdevkit/\"+testOrTrain, minSizePercent, maxSizePercent, CLASSES)\n",
    "  # Figure out a file name based on the current iteration and type of dataset\n",
    "  baseFilename = f\"{testOrTrain}_{i:0{6}d}\" # Max 100000 file names\n",
    "  # Save the image to the specified folder based on type of data set and use the above created filename\n",
    "  combinedImage.save('./ultralytics/Dataset/images/'+testOrTrain+'/'+baseFilename+'.png')\n",
    "\n",
    "  # Open/create a text file with the same name as the image and add the paste_parameters_yolo to it\n",
    "  with open('./ultralytics/Dataset/labels/'+testOrTrain+'/'+baseFilename+'.txt', \"w\") as f:\n",
    "    yoloData = pasteParametersYolo\n",
    "    if round(yoloData['x_center'],6) > 1 or round(yoloData['y_center'],6) > 1:\n",
    "      print(\"ERROR at\", i)\n",
    "      print(yoloData)\n",
    "      print(debugParameters)\n",
    "    f.write(f\"{yoloData['objectClassNum']} {round(yoloData['x_center'],6)} {round(yoloData['y_center'],6)} {round(yoloData['width'],6)} {round(yoloData['height'],6)}\") # write data and round it to the correct decimal places (first digit in string is the class)\n",
    "\n",
    "\n",
    "\n",
    "# Main function that ties together all of the other functions to make it work\n",
    "def makeData(testOrTrain=\"test\", numImages=10, minSizePercent=.05, maxSizePercent=.8):\n",
    "  args = tuple([(testOrTrain, numImages, minSizePercent, maxSizePercent, CLASSES, i) for i in range(numImages)]) # creates a tuple of tuple\n",
    "\n",
    "  with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    executor.map(makeImage, args)\n",
    "    print(\"yippie!!! it works\") # to make sure it works :)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\"--classes\", type=str)\n",
    "  parser.add_argument(\"--testOrTrain\", default=\"test\", type=str)\n",
    "  parser.add_argument(\"--numImages\", default=2, type=int)\n",
    "  parser.add_argument(\"--minSizePercent\", default=.05, type=float)\n",
    "  parser.add_argument(\"--maxSizePercent\", default=.8, type=float)\n",
    "\n",
    "  inputArgs = parser.parse_args()\n",
    "\n",
    "  CLASSES = json.loads(inputArgs.classes.replace(\"'\", '\"'))\n",
    "  testOrTrain = inputArgs.testOrTrain\n",
    "  numImages = inputArgs.numImages\n",
    "  minSizePercent = inputArgs.minSizePercent\n",
    "  maxSizePercent = inputArgs.maxSizePercent\n",
    "\n",
    "  testOrTrain = testOrTrain.lower()\n",
    "\n",
    "  makeData(testOrTrain, numImages, minSizePercent, maxSizePercent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the test and train image sets. You should pick a size percent that makes sense for the application. I suggest doing some tests by just generating 50 images and then seeing how it looks. Here are the lines to make a test and train dataset:\n",
    "\n",
    "(This is going to take a few minutes on some machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train code\n",
    "os.chdir(absoluteMVpath + \"/ultralytics\")\n",
    "!python3 dataGen.py --classes=\"$CLASSES\" --testOrTrain=\"train\" --numImages=15000 --minSizePercent=.1 --maxSizePercent=.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "os.chdir(absoluteMVpath + \"/ultralytics\")\n",
    "!python3 dataGen.py --classes=\"$CLASSES\" --testOrTrain=\"test\" --numImages=7500 --minSizePercent=.1 --maxSizePercent=.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the dataset config file for training yolo but **make sure to enumerate the classes at the bottom and pick names that are appropriate**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./ultralytics/cfg/datasets/CUSTOM.yaml\n",
    "# Ultralytics YOLO ðŸš€, AGPL-3.0 license\n",
    "# Documentation: # Documentation: https://docs.ultralytics.com/datasets/detect/voc/\n",
    "# Example usage: yolo train data=CUSTOM.yaml\n",
    "# parent\n",
    "# â”œâ”€â”€ ultralytics\n",
    "# â””â”€â”€ datasets\n",
    "#     â””â”€â”€ CUSTOM\n",
    "\n",
    "# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\n",
    "path: ./ultralytics/Dataset/\n",
    "train: images/train # train images (relative to 'path')  16551 images\n",
    "val: images/test # val images (relative to 'path')  4952 images\n",
    "test: images/test # test images (optional)\n",
    "\n",
    "# Set the number of classes\n",
    "nc: 1 # EDIT TO REFLECT CORRECT VALUES\n",
    "\n",
    "# Classes\n",
    "names: # EDIT HERE\n",
    "  0: algae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model\n",
    "\n",
    "Now that we have the dataset we can train the model. First we will run the checks command to make sure everything is good (it has all the libraries, the computer is able to train it, etc).\n",
    "We are going to use the nano version of yolov11 because it is the smallest, fastest, and most lightweight version. We will specify the name of the custom data config file that we created above as well as yolov11n.pt for the nano version of the pretrained model. I've decided on 10 epochs for just a short retraining of the model. The imgsz is 640 which will resize all of the images we created to 640x640 for input into the model. This number can be changed for speed over accuracy if needed. I set the batch size to use 90% of the available GPU memory as well as set cache to true to improve speed.\n",
    "\n",
    "--------\n",
    "\n",
    "my face when yolov12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo checks\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "!yolo help\n",
    "!yolo settings datasets_dir=ultralytics\n",
    "# i am a genuous....\n",
    "from ultralytics import settings\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imgsz is set to 512 because it must be a multiple of 32 and not greater than 530 to work with the camera with the OV... sensor\n",
    "\n",
    "KEEP IN MIND:\n",
    "if you didn't already know, training takes a while (as in I started training at around 1 am. it is now 8 am). Depending on what system you're on, it will probably take less long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isCuda:\n",
    "    isCuda = True\n",
    "    !yolo detect train data=\"./ultralytics/cfg/datasets/CUSTOM.yaml\" model=\"yolo12n.pt\" epochs=2 imgsz=512 name=\"lakeMONSTER\" batch=0.5 patience=2 cache=disk workers=8 device=0 project=./datagenTraining/\n",
    "else:\n",
    "    isCuda = False\n",
    "    !yolo detect train data=\"./ultralytics/cfg/datasets/CUSTOM.yaml\" model=\"yolo12n.pt\" epochs=2 imgsz=512 name=\"lakeMONSTER\" batch=0.5 patience=2 cache=disk workers=8 device=cpu project=./datagenTraining/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Upload to Luxonis to convert to OpenVINO format\n",
    "\n",
    "### NOTE: tools.luxonis.com is no longer maintained and is not compatible with versions past Yolov11 or depthAIv3.\n",
    "(How unfortunate that those are both versions we want to upgrade our code to...)\n",
    "\n",
    "-------\n",
    "\n",
    "Navigate to [tools.luxonis.com](https://tools.luxonis.com) and select the following parameters:\n",
    "\n",
    "- Yolo Version: `YOLOv11`\n",
    "- `RVC2`\n",
    "- Upload your best.pt file (when you finish running the above code it prints where the files are)\n",
    "  - We use best.pt because it is literally the best out of the rest... last.pt is just the last trained epoch, since there may be situations where too much training actually makes it worse, so the best isn't one of the last few epochs. Most of the time they're pretty similar but we should use best.pt\n",
    "- Image shape: `512`\n",
    "- Shaves: `6`\n",
    "- Use OpenVINO: `True`\n",
    "\n",
    "When it finishes processing, you will get a `result.zip` file. Open it and replace everything in the `models` folder with the content of the `result` folder.\n",
    "\n",
    "## End\n",
    "You are done with this notebook! The next steps of using the models come in after you set up MonsterVision on the robot. Refer to the README.md for more clear instructions on when the models come in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
